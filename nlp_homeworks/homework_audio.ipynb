{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/germanjke/Deep_Learning_School_MIPT/blob/master/nlp_homeworks/homework_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zahzrEdRCaxV"
      },
      "source": [
        "### Spoken Language Processing\n",
        "Homework 1\n",
        "1. Ссылка на занятие: https://www.twitch.tv/deeplearningschool\n",
        "2. Материалы: https://vk.cc/bVLCOC https://vk.cc/bVLD3Z\n",
        "\n",
        "В этом задании предлагается обучить классификатор класса возраста по голосу (пример с тем, как это можно сделать для пола см. в семинаре)\n",
        "\n",
        "P.S. не забудьте, что если то вы работает в Colab, то вы можете поменять среду выполнения на GPU/TPU!\n",
        "\n",
        "Вопросы по заданию/материалам: @Nestyme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wSgHrbiEc8x",
        "outputId": "b46687ae-ccfe-4135-dc65-aac6dc420e63"
      },
      "source": [
        "!pip3 install timit-utils==0.9.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timit-utils==0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/22/32/0c98f7f44386947b9e4080f54f09a7380c390e0b8337ab0b87050d49c43a/timit_utils-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.19.4)\n",
            "Collecting python-speech-features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.1.5)\n",
            "Collecting SoundFile>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->timit-utils==0.9.0) (2018.9)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile>=0.8.0->timit-utils==0.9.0) (1.14.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->timit-utils==0.9.0) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile>=0.8.0->timit-utils==0.9.0) (2.20)\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5890 sha256=2c62570f812fed7e25127b2cec75a4948e11e57a082736ceaa2f19edee3bc00f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features, SoundFile, timit-utils\n",
            "Successfully installed SoundFile-0.10.3.post1 python-speech-features-0.6 timit-utils-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PveQyMmsErXM",
        "outputId": "d7824cf9-4f0e-4b2c-ff7a-2e93081d392c"
      },
      "source": [
        "!pip3 install torchaudio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/f9/618434cf4e46dc975871e1516f5499abef6564ab4366f9b2321ee536be14/torchaudio-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 11.8MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (0.8)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed torch-1.7.1 torchaudio-0.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifZp0NvTEo8V",
        "outputId": "9c444d21-0e80-4f7e-a13b-b57d05884eb6"
      },
      "source": [
        "! wget https://ndownloader.figshare.com/files/10256148 "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-27 11:49:37--  https://ndownloader.figshare.com/files/10256148\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.248.20.23, 34.255.68.79, 52.18.110.156, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.248.20.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10256148/TIMIT.zip [following]\n",
            "--2020-12-27 11:49:38--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10256148/TIMIT.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.36.146\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 440207227 (420M) [binary/octet-stream]\n",
            "Saving to: ‘10256148’\n",
            "\n",
            "10256148            100%[===================>] 419.81M  34.3MB/s    in 13s     \n",
            "\n",
            "2020-12-27 11:49:51 (32.3 MB/s) - ‘10256148’ saved [440207227/440207227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W401P4FhEq0c"
      },
      "source": [
        "!unzip -q 10256148"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0bovLZ0Ew5V"
      },
      "source": [
        "import timit_utils as tu\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import IPython\n",
        "_TIMIT_PATH = 'data/lisa/data/timit/raw/TIMIT'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd-qfC9-DdnJ"
      },
      "source": [
        "## Задание 1\n",
        "Загрузите данные для обучения. Для этого:\n",
        "1. Скачайте датасет TIMIT (см семинар)\n",
        "2. Соберите пары \"голос\"  — \"класс возраста\" также, как на семинаре собирались пары \"голос\"  — \"пол\". Аудиодорожки сконвертируйте в мелспектрограммы при помощи `torchaudio либо` `librosa`\n",
        "\n",
        "P.S. вы можете использовать свою реализацию, а можете предложенную (см следующие ячейки)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhPyP4T5DdAD"
      },
      "source": [
        "import timit_utils as tu\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch as t\n",
        "\n",
        "\n",
        "class timit_dataloader:\n",
        "    def __init__(self, data_path=_TIMIT_PATH, train_mode=True, age_mode=True):\n",
        "        self.doc_file_path = os.path.join(data_path, 'DOC', 'SPKRINFO.TXT')\n",
        "        self.corpus = tu.Corpus(data_path)\n",
        "        with open(self.doc_file_path) as f:\n",
        "            self.id_age_dict = dict(\n",
        "                [(tmp.split(' ')[0], 86 - int(tmp.split('  ')[5].split('/')[-1].replace('??', '50'))) \\\n",
        "                 for tmp in f.readlines()[39:]])\n",
        "        if train_mode:\n",
        "            self.trainset = self.create_dataset('train', age_mode=age_mode)\n",
        "            self.validset = self.create_dataset('valid', age_mode=age_mode)\n",
        "        self.testset = self.create_dataset('test', age_mode=age_mode)\n",
        "\n",
        "    def return_age(self, id):\n",
        "        return self.id_age_dict[id]\n",
        "\n",
        "    def return_data(self):\n",
        "        return self.trainset, self.validset, self.testset\n",
        "\n",
        "    def return_test(self):\n",
        "        return self.testset\n",
        "\n",
        "    def create_dataset(self, mode, age_mode=False):\n",
        "        global people\n",
        "        assert mode in ['train', 'valid', 'test']\n",
        "        if mode == 'train':\n",
        "            people = [self.corpus.train.person_by_index(i) for i in range(350)]\n",
        "        if mode == 'valid':\n",
        "            people = [self.corpus.train.person_by_index(i) for i in range(350, 400)]\n",
        "        if mode == 'test':\n",
        "            people = [self.corpus.test.person_by_index(i) for i in range(150)]\n",
        "        spectrograms_and_targets = []\n",
        "        for person in tqdm(people):\n",
        "              try:\n",
        "                  target = self.return_age(person.name)\n",
        "                  for i in range(len(person.sentences)):\n",
        "                      spectrograms_and_targets.append(\n",
        "                          self.preprocess_sample(person.sentence_by_index(i).raw_audio, target, age_mode=True))\n",
        "              except:\n",
        "                  print(person.name, target)\n",
        "\n",
        "        X, y = map(np.stack, zip(*spectrograms_and_targets))\n",
        "        X = X.transpose([0, 2, 1])  # to [batch, time, channels]\n",
        "        return X, y\n",
        "\n",
        "    @staticmethod\n",
        "    def spec_to_image(spec, eps=1e-6):\n",
        "        mean = spec.mean()\n",
        "        std = spec.std()\n",
        "        spec_norm = (spec - mean) / (std + eps)\n",
        "        spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "        spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "        spec_scaled = spec_scaled.astype(np.uint8)\n",
        "        return spec_scaled\n",
        "\n",
        "    @staticmethod\n",
        "    def clasterize_by_age(age):\n",
        "        if age < 25:\n",
        "            return 0\n",
        "        if 25 <= age <= 40:\n",
        "            return 1\n",
        "        if age > 40:\n",
        "            return 2\n",
        "\n",
        "    def preprocess_sample(self, amplitudes, target, age_mode=False, sr=16000, max_length=150):\n",
        "        spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr, n_mels=128, fmin=1, fmax=8192)[:, :max_length]\n",
        "        spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "        target = self.clasterize_by_age(target)\n",
        "        return self.spec_to_image(np.float32(spectrogram)), target\n",
        "\n",
        "    def preprocess_sample_inference(self, amplitudes, sr=16000, max_length=150, device='cpu'):\n",
        "        spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr, n_mels=128, fmin=1, fmax=8192)[:, :max_length]\n",
        "        spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "        spectrogram = np.array([self.spec_to_image(np.float32(spectrogram))]).transpose([0, 2, 1])\n",
        "\n",
        "        return t.tensor(spectrogram, dtype=t.float).to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class dataloader:\n",
        "    def __init__(self, spectrograms, targets):\n",
        "        self.data = list(zip(spectrograms, targets))\n",
        "\n",
        "    def next_batch(self, batch_size, device):\n",
        "        indices = np.random.randint(len(self.data), size=batch_size)\n",
        "\n",
        "        input = [self.data[i] for i in indices]\n",
        "\n",
        "        source = [line[0] for line in input]\n",
        "        target = [line[1] for line in input]\n",
        "\n",
        "        return self.torch_batch(source, target, device)\n",
        "\n",
        "    @staticmethod\n",
        "    def torch_batch(source, target, device):\n",
        "        return tuple(\n",
        "            [\n",
        "                t.tensor(val, dtype=t.float).to(device, non_blocking=True)\n",
        "                for val in [source, target]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def padd_sequences(lines, pad_token=0):\n",
        "        lengths = [len(line) for line in lines]\n",
        "        max_length = max(lengths)\n",
        "\n",
        "        return np.array(\n",
        "            [\n",
        "                line + [pad_token] * (max_length - lengths[i])\n",
        "                for i, line in enumerate(lines)\n",
        "            ]\n",
        "        )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpz1Q5VOFxLM"
      },
      "source": [
        "Простая сверточная сеть, ее можно дотюнить или поменять по желанию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF9fIVq7Dbwx"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, window_sizes=(3, 4, 5)):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, 128, [window_size, 128], padding=(window_size - 1, 0))\n",
        "            for window_size in window_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(128 * len(window_sizes), 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply a convolution + max pool layer for each window size\n",
        "        x = torch.unsqueeze(x, 1)  # [B, C, T, E] Add a channel dim.\n",
        "        xs = []\n",
        "        for conv in self.convs:\n",
        "            x2 = F.relu(conv(x))  # [B, F, T, 1]\n",
        "            x2 = torch.squeeze(x2, -1)  # [B, F, T]\n",
        "            x2 = F.max_pool1d(x2, x2.size(2))  # [B, F, 1]\n",
        "            xs.append(x2)\n",
        "        x = torch.cat(xs, 2)  # [B, F, window]\n",
        "\n",
        "        # FC\n",
        "        x = x.view(x.size(0), -1)  # [B, F * window]\n",
        "        logits = self.fc(x)  # [B, class]\n",
        "        #probs = torch.sigmoid(logits).view(-1)\n",
        "        return logits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EoOR641Fkzx",
        "outputId": "25ab344c-02e2-472b-c733-86a95a3561f1"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'using {device} mode')\n",
        "patience = 500\n",
        "best_loss = 1000\n",
        "cnt = 0\n",
        "\n",
        "model = Model()\n",
        "if device == torch.device('cuda'):\n",
        "    model.cuda()\n",
        "else:\n",
        "    model.cpu()\n",
        "model.train()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cpu mode\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 128, kernel_size=[3, 128], stride=(1, 1), padding=(2, 0))\n",
              "    (1): Conv2d(1, 128, kernel_size=[4, 128], stride=(1, 1), padding=(3, 0))\n",
              "    (2): Conv2d(1, 128, kernel_size=[5, 128], stride=(1, 1), padding=(4, 0))\n",
              "  )\n",
              "  (fc): Linear(in_features=384, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLUggB9iF6s_",
        "outputId": "930f30bf-f34a-458e-8143-95d8ede94f03"
      },
      "source": [
        "_timit_dataloader = timit_dataloader()\n",
        "train, valid, test = _timit_dataloader.return_data()\n",
        "\n",
        "trainset = dataloader(*train)\n",
        "validset = dataloader(*valid)\n",
        "testset = dataloader(*test)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "optimizer = Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad], betas=(0.9, 0.999), eps=1e-5\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 350/350 [00:41<00:00,  8.36it/s]\n",
            "100%|██████████| 50/50 [00:06<00:00,  8.23it/s]\n",
            "100%|██████████| 150/150 [00:17<00:00,  8.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScCZEMvXHkmz"
      },
      "source": [
        "#Задание 2\n",
        "1. Обучите свой классификатор категории возраста\n",
        "2. Попробуйте улучшить результат. Можно попробовать усложнить сетку, подвигать границы категорий, поискать новые данные, что угодно, кроме учиться на тесте :)\n",
        "3. Какой подход оказался самым эффективным? Как думаете, почему?\n",
        "4. Как считаете, где можно было бы применить такой классификатор в качестве вспомогательной задачи?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igQT2jOFis2q",
        "outputId": "4c8594fd-fa7d-4012-c4c3-227cdfb3037f"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "x = []\n",
        "outs_to_f1 = np.array(x)\n",
        "\n",
        "x = []\n",
        "targets_to_f1 = np.array(x)\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input, target = trainset.next_batch(BATCH_SIZE, device=device)\n",
        "    out = model(input)\n",
        "    loss = criterion(out, target.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 25 == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input, target = validset.next_batch(BATCH_SIZE, device=device)\n",
        "            out = model(input)\n",
        "            out_ = m(out)\n",
        "            out_ = torch.argmax(out_, dim=1)\n",
        "            valid_loss =  criterion(out, target.long())\n",
        "            out, target = out_.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
        "\n",
        "            outs_to_f1 = np.concatenate((outs_to_f1, out), axis=None)\n",
        "            targets_to_f1 = np.concatenate((targets_to_f1, target), axis=None) # для проверки качества\n",
        "\n",
        "            print(\"iteration {}, valid_loss {}\".format(i, valid_loss.item()))\n",
        "\n",
        "        model.train()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0, valid_loss 0.7826313376426697\n",
            "iteration 25, valid_loss 1.5825917720794678\n",
            "iteration 50, valid_loss 2.150235176086426\n",
            "iteration 75, valid_loss 1.9359036684036255\n",
            "iteration 100, valid_loss 1.2500444650650024\n",
            "iteration 125, valid_loss 2.0892398357391357\n",
            "iteration 150, valid_loss 1.6511121988296509\n",
            "iteration 175, valid_loss 1.2036471366882324\n",
            "iteration 200, valid_loss 1.9425990581512451\n",
            "iteration 225, valid_loss 1.4751800298690796\n",
            "iteration 250, valid_loss 1.330794334411621\n",
            "iteration 275, valid_loss 1.4080251455307007\n",
            "iteration 300, valid_loss 1.5300043821334839\n",
            "iteration 325, valid_loss 1.2561153173446655\n",
            "iteration 350, valid_loss 2.206315755844116\n",
            "iteration 375, valid_loss 1.627901554107666\n",
            "iteration 400, valid_loss 1.766798734664917\n",
            "iteration 425, valid_loss 1.2091641426086426\n",
            "iteration 450, valid_loss 2.1302788257598877\n",
            "iteration 475, valid_loss 1.8829354047775269\n",
            "iteration 500, valid_loss 1.8282757997512817\n",
            "iteration 525, valid_loss 1.4080092906951904\n",
            "iteration 550, valid_loss 1.4117076396942139\n",
            "iteration 575, valid_loss 1.5094058513641357\n",
            "iteration 600, valid_loss 2.407684326171875\n",
            "iteration 625, valid_loss 2.5683507919311523\n",
            "iteration 650, valid_loss 1.7578405141830444\n",
            "iteration 675, valid_loss 1.55852210521698\n",
            "iteration 700, valid_loss 1.4178146123886108\n",
            "iteration 725, valid_loss 2.6028056144714355\n",
            "iteration 750, valid_loss 2.164797067642212\n",
            "iteration 775, valid_loss 2.5910418033599854\n",
            "iteration 800, valid_loss 2.22912335395813\n",
            "iteration 825, valid_loss 1.7350767850875854\n",
            "iteration 850, valid_loss 2.02229642868042\n",
            "iteration 875, valid_loss 2.2953710556030273\n",
            "iteration 900, valid_loss 1.9701260328292847\n",
            "iteration 925, valid_loss 2.6415855884552\n",
            "iteration 950, valid_loss 2.2072057723999023\n",
            "iteration 975, valid_loss 2.4312963485717773\n",
            "iteration 1000, valid_loss 1.9247103929519653\n",
            "iteration 1025, valid_loss 1.6979435682296753\n",
            "iteration 1050, valid_loss 1.538610577583313\n",
            "iteration 1075, valid_loss 1.8318971395492554\n",
            "iteration 1100, valid_loss 1.1933131217956543\n",
            "iteration 1125, valid_loss 2.135333776473999\n",
            "iteration 1150, valid_loss 2.3693244457244873\n",
            "iteration 1175, valid_loss 1.7610814571380615\n",
            "iteration 1200, valid_loss 2.218994617462158\n",
            "iteration 1225, valid_loss 2.015514850616455\n",
            "iteration 1250, valid_loss 2.213914632797241\n",
            "iteration 1275, valid_loss 1.9535937309265137\n",
            "iteration 1300, valid_loss 2.4621431827545166\n",
            "iteration 1325, valid_loss 2.320173740386963\n",
            "iteration 1350, valid_loss 1.639060378074646\n",
            "iteration 1375, valid_loss 2.300717353820801\n",
            "iteration 1400, valid_loss 2.7629146575927734\n",
            "iteration 1425, valid_loss 1.2452526092529297\n",
            "iteration 1450, valid_loss 2.7466306686401367\n",
            "iteration 1475, valid_loss 1.9301071166992188\n",
            "iteration 1500, valid_loss 2.6380531787872314\n",
            "iteration 1525, valid_loss 2.819319486618042\n",
            "iteration 1550, valid_loss 2.528322696685791\n",
            "iteration 1575, valid_loss 2.5804524421691895\n",
            "iteration 1600, valid_loss 1.5504072904586792\n",
            "iteration 1625, valid_loss 2.3837485313415527\n",
            "iteration 1650, valid_loss 2.187666654586792\n",
            "iteration 1675, valid_loss 1.4046145677566528\n",
            "iteration 1700, valid_loss 3.219686508178711\n",
            "iteration 1725, valid_loss 1.6595765352249146\n",
            "iteration 1750, valid_loss 2.259272813796997\n",
            "iteration 1775, valid_loss 1.5474470853805542\n",
            "iteration 1800, valid_loss 2.2979557514190674\n",
            "iteration 1825, valid_loss 2.1460297107696533\n",
            "iteration 1850, valid_loss 2.301499843597412\n",
            "iteration 1875, valid_loss 1.7561824321746826\n",
            "iteration 1900, valid_loss 1.6477216482162476\n",
            "iteration 1925, valid_loss 2.639981985092163\n",
            "iteration 1950, valid_loss 1.9520251750946045\n",
            "iteration 1975, valid_loss 3.2255749702453613\n",
            "iteration 2000, valid_loss 2.4274027347564697\n",
            "iteration 2025, valid_loss 2.2352428436279297\n",
            "iteration 2050, valid_loss 2.4586141109466553\n",
            "iteration 2075, valid_loss 2.537111759185791\n",
            "iteration 2100, valid_loss 2.791701555252075\n",
            "iteration 2125, valid_loss 2.5224804878234863\n",
            "iteration 2150, valid_loss 1.7232742309570312\n",
            "iteration 2175, valid_loss 2.4109857082366943\n",
            "iteration 2200, valid_loss 2.4794445037841797\n",
            "iteration 2225, valid_loss 2.7558252811431885\n",
            "iteration 2250, valid_loss 2.04825758934021\n",
            "iteration 2275, valid_loss 2.5215840339660645\n",
            "iteration 2300, valid_loss 3.0704643726348877\n",
            "iteration 2325, valid_loss 2.2331557273864746\n",
            "iteration 2350, valid_loss 3.184723377227783\n",
            "iteration 2375, valid_loss 2.2593870162963867\n",
            "iteration 2400, valid_loss 1.9851179122924805\n",
            "iteration 2425, valid_loss 2.5070223808288574\n",
            "iteration 2450, valid_loss 1.9537605047225952\n",
            "iteration 2475, valid_loss 1.3350931406021118\n",
            "iteration 2500, valid_loss 2.690516948699951\n",
            "iteration 2525, valid_loss 2.6550979614257812\n",
            "iteration 2550, valid_loss 2.9854536056518555\n",
            "iteration 2575, valid_loss 3.1279118061065674\n",
            "iteration 2600, valid_loss 2.632070302963257\n",
            "iteration 2625, valid_loss 2.4077141284942627\n",
            "iteration 2650, valid_loss 2.268279790878296\n",
            "iteration 2675, valid_loss 3.029724359512329\n",
            "iteration 2700, valid_loss 2.9889092445373535\n",
            "iteration 2725, valid_loss 2.6404154300689697\n",
            "iteration 2750, valid_loss 1.9638696908950806\n",
            "iteration 2775, valid_loss 2.9642722606658936\n",
            "iteration 2800, valid_loss 2.397071361541748\n",
            "iteration 2825, valid_loss 2.675156354904175\n",
            "iteration 2850, valid_loss 2.5950708389282227\n",
            "iteration 2875, valid_loss 1.6437175273895264\n",
            "iteration 2900, valid_loss 3.598262071609497\n",
            "iteration 2925, valid_loss 2.684377670288086\n",
            "iteration 2950, valid_loss 2.4015707969665527\n",
            "iteration 2975, valid_loss 2.4484031200408936\n",
            "iteration 3000, valid_loss 2.445432662963867\n",
            "iteration 3025, valid_loss 2.680950880050659\n",
            "iteration 3050, valid_loss 2.1881885528564453\n",
            "iteration 3075, valid_loss 1.8814477920532227\n",
            "iteration 3100, valid_loss 2.5817508697509766\n",
            "iteration 3125, valid_loss 2.2271502017974854\n",
            "iteration 3150, valid_loss 2.255878210067749\n",
            "iteration 3175, valid_loss 2.7749552726745605\n",
            "iteration 3200, valid_loss 2.146944284439087\n",
            "iteration 3225, valid_loss 3.492518901824951\n",
            "iteration 3250, valid_loss 2.214700222015381\n",
            "iteration 3275, valid_loss 2.676093101501465\n",
            "iteration 3300, valid_loss 2.969224214553833\n",
            "iteration 3325, valid_loss 2.1197690963745117\n",
            "iteration 3350, valid_loss 3.1257498264312744\n",
            "iteration 3375, valid_loss 1.997540831565857\n",
            "iteration 3400, valid_loss 2.383329391479492\n",
            "iteration 3425, valid_loss 2.6592135429382324\n",
            "iteration 3450, valid_loss 2.729624032974243\n",
            "iteration 3475, valid_loss 2.7659990787506104\n",
            "iteration 3500, valid_loss 2.4594805240631104\n",
            "iteration 3525, valid_loss 3.1706998348236084\n",
            "iteration 3550, valid_loss 2.6014182567596436\n",
            "iteration 3575, valid_loss 1.6249547004699707\n",
            "iteration 3600, valid_loss 3.4750306606292725\n",
            "iteration 3625, valid_loss 2.45912504196167\n",
            "iteration 3650, valid_loss 2.330246925354004\n",
            "iteration 3675, valid_loss 3.6616039276123047\n",
            "iteration 3700, valid_loss 2.588545560836792\n",
            "iteration 3725, valid_loss 1.4670203924179077\n",
            "iteration 3750, valid_loss 1.9342776536941528\n",
            "iteration 3775, valid_loss 2.4253523349761963\n",
            "iteration 3800, valid_loss 3.478767156600952\n",
            "iteration 3825, valid_loss 1.8363194465637207\n",
            "iteration 3850, valid_loss 4.075451850891113\n",
            "iteration 3875, valid_loss 3.010585308074951\n",
            "iteration 3900, valid_loss 2.7324841022491455\n",
            "iteration 3925, valid_loss 2.6752357482910156\n",
            "iteration 3950, valid_loss 2.855668544769287\n",
            "iteration 3975, valid_loss 3.2024524211883545\n",
            "iteration 4000, valid_loss 2.0660438537597656\n",
            "iteration 4025, valid_loss 2.756807804107666\n",
            "iteration 4050, valid_loss 3.1640892028808594\n",
            "iteration 4075, valid_loss 2.5524559020996094\n",
            "iteration 4100, valid_loss 3.4010417461395264\n",
            "iteration 4125, valid_loss 2.7256641387939453\n",
            "iteration 4150, valid_loss 2.6902573108673096\n",
            "iteration 4175, valid_loss 2.500033378601074\n",
            "iteration 4200, valid_loss 3.674163818359375\n",
            "iteration 4225, valid_loss 1.8688507080078125\n",
            "iteration 4250, valid_loss 2.326949119567871\n",
            "iteration 4275, valid_loss 2.0214104652404785\n",
            "iteration 4300, valid_loss 2.867631196975708\n",
            "iteration 4325, valid_loss 2.6326870918273926\n",
            "iteration 4350, valid_loss 3.2418947219848633\n",
            "iteration 4375, valid_loss 2.2408840656280518\n",
            "iteration 4400, valid_loss 2.935607671737671\n",
            "iteration 4425, valid_loss 3.1951346397399902\n",
            "iteration 4450, valid_loss 2.1170003414154053\n",
            "iteration 4475, valid_loss 1.9092590808868408\n",
            "iteration 4500, valid_loss 2.601377248764038\n",
            "iteration 4525, valid_loss 2.5092835426330566\n",
            "iteration 4550, valid_loss 2.603148937225342\n",
            "iteration 4575, valid_loss 2.85343599319458\n",
            "iteration 4600, valid_loss 3.2612838745117188\n",
            "iteration 4625, valid_loss 2.5200891494750977\n",
            "iteration 4650, valid_loss 1.853529930114746\n",
            "iteration 4675, valid_loss 1.07750403881073\n",
            "iteration 4700, valid_loss 3.235602378845215\n",
            "iteration 4725, valid_loss 3.5733108520507812\n",
            "iteration 4750, valid_loss 1.9872938394546509\n",
            "iteration 4775, valid_loss 3.209583282470703\n",
            "iteration 4800, valid_loss 3.052978515625\n",
            "iteration 4825, valid_loss 3.215585470199585\n",
            "iteration 4850, valid_loss 2.9640939235687256\n",
            "iteration 4875, valid_loss 2.3138742446899414\n",
            "iteration 4900, valid_loss 2.246034622192383\n",
            "iteration 4925, valid_loss 2.5586981773376465\n",
            "iteration 4950, valid_loss 3.524458885192871\n",
            "iteration 4975, valid_loss 2.7865006923675537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PCsaur-LthD"
      },
      "source": [
        "Метрикой для проверки точности классификатора была выбрана F1. Метрика считается на val датасете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQNYfqaXGXN4",
        "outputId": "50c5e610-cd0e-4e21-9ff8-7b558992adeb"
      },
      "source": [
        "f1_score(outs_to_f1, targets_to_f1, average='micro')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6534375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Su_WC7SdOwA"
      },
      "source": [
        "А теперь метрика на test датасете"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV8XfAKoV049"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "x = []\n",
        "outs_to_f1_test = np.array(x)\n",
        "\n",
        "x = []\n",
        "targets_to_f1_test = np.array(x)\n",
        "\n",
        "for i in range(5000):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input, target = testset.next_batch(BATCH_SIZE, device=device)\n",
        "        out = model(input)\n",
        "        out_ = m(out)\n",
        "        out_ = torch.argmax(out_, dim=1)\n",
        "        valid_loss =  criterion(out, target.long())\n",
        "        out, target = out_.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
        "\n",
        "        outs_to_f1_test = np.concatenate((outs_to_f1_test, out), axis=None)\n",
        "        targets_to_f1_test = np.concatenate((targets_to_f1_test, target), axis=None) # для проверки качества"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3UyJSPOWYcY",
        "outputId": "32129b8c-aedd-4091-d2f6-ea3cea01b542"
      },
      "source": [
        "f1_score(outs_to_f1_test, targets_to_f1_test, average='micro')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.63118125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqo2vxpgPD6B"
      },
      "source": [
        "Как увеличить точность? Самый оптимальный вариант - увеличить данные. Я, правда, не очень знаю, где найти их :( "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeiTEV59euWH"
      },
      "source": [
        "Попробуем поиграться с шедулерами.\n",
        "\n",
        "Как завещал Павел Остяков в докладе \"как выигрывать кагл\" - шедулеры это важно. Чтож, попробуем..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRm2PB2tgE6H"
      },
      "source": [
        "Рассмотрим следующие шедулеры:\n",
        "\n",
        "\n",
        "*   StepLR\n",
        "*   ReduceLROnPlateau\n",
        "\n",
        "Проверим их на валидиации, с той же метрикой F1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTMbX6auW6zp"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41wO1bBZeIOI",
        "outputId": "f590a51f-fba4-4fe3-fa2e-d45d8815bc9e"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "x = []\n",
        "outs_to_f1 = np.array(x)\n",
        "\n",
        "x = []\n",
        "targets_to_f1 = np.array(x)\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input, target = trainset.next_batch(BATCH_SIZE, device=device)\n",
        "    out = model(input)\n",
        "    loss = criterion(out, target.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i % 25 == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input, target = validset.next_batch(BATCH_SIZE, device=device)\n",
        "            out = model(input)\n",
        "            out_ = m(out)\n",
        "            out_ = torch.argmax(out_, dim=1)\n",
        "            valid_loss =  criterion(out, target.long())\n",
        "            out, target = out_.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
        "\n",
        "            outs_to_f1 = np.concatenate((outs_to_f1, out), axis=None)\n",
        "            targets_to_f1 = np.concatenate((targets_to_f1, target), axis=None) # для проверки качества\n",
        "\n",
        "            print(\"iteration {}, valid_loss {}\".format(i, valid_loss.item()))\n",
        "\n",
        "        model.train()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0, valid_loss 2.2966086864471436\n",
            "iteration 25, valid_loss 5.340365409851074\n",
            "iteration 50, valid_loss 3.0654983520507812\n",
            "iteration 75, valid_loss 3.255983829498291\n",
            "iteration 100, valid_loss 3.137010335922241\n",
            "iteration 125, valid_loss 2.374645948410034\n",
            "iteration 150, valid_loss 3.081944227218628\n",
            "iteration 175, valid_loss 1.7752131223678589\n",
            "iteration 200, valid_loss 2.6618635654449463\n",
            "iteration 225, valid_loss 3.1778430938720703\n",
            "iteration 250, valid_loss 2.621580123901367\n",
            "iteration 275, valid_loss 3.8967695236206055\n",
            "iteration 300, valid_loss 2.64426851272583\n",
            "iteration 325, valid_loss 4.332491397857666\n",
            "iteration 350, valid_loss 2.3346898555755615\n",
            "iteration 375, valid_loss 2.220642328262329\n",
            "iteration 400, valid_loss 2.2223293781280518\n",
            "iteration 425, valid_loss 1.7765028476715088\n",
            "iteration 450, valid_loss 3.6253929138183594\n",
            "iteration 475, valid_loss 3.312833547592163\n",
            "iteration 500, valid_loss 3.1469943523406982\n",
            "iteration 525, valid_loss 3.9852426052093506\n",
            "iteration 550, valid_loss 2.7920334339141846\n",
            "iteration 575, valid_loss 2.332871913909912\n",
            "iteration 600, valid_loss 2.0950701236724854\n",
            "iteration 625, valid_loss 3.0127813816070557\n",
            "iteration 650, valid_loss 3.1860568523406982\n",
            "iteration 675, valid_loss 2.409228801727295\n",
            "iteration 700, valid_loss 2.452526092529297\n",
            "iteration 725, valid_loss 3.436232805252075\n",
            "iteration 750, valid_loss 3.3568692207336426\n",
            "iteration 775, valid_loss 3.331066131591797\n",
            "iteration 800, valid_loss 3.487415075302124\n",
            "iteration 825, valid_loss 2.6748385429382324\n",
            "iteration 850, valid_loss 3.411817789077759\n",
            "iteration 875, valid_loss 3.528381586074829\n",
            "iteration 900, valid_loss 2.9611549377441406\n",
            "iteration 925, valid_loss 2.7505316734313965\n",
            "iteration 950, valid_loss 3.056879758834839\n",
            "iteration 975, valid_loss 3.108375072479248\n",
            "iteration 1000, valid_loss 2.778742551803589\n",
            "iteration 1025, valid_loss 2.698073387145996\n",
            "iteration 1050, valid_loss 2.2992773056030273\n",
            "iteration 1075, valid_loss 2.5591320991516113\n",
            "iteration 1100, valid_loss 2.6641759872436523\n",
            "iteration 1125, valid_loss 3.784928798675537\n",
            "iteration 1150, valid_loss 1.9269375801086426\n",
            "iteration 1175, valid_loss 2.8195629119873047\n",
            "iteration 1200, valid_loss 2.2229039669036865\n",
            "iteration 1225, valid_loss 3.096940755844116\n",
            "iteration 1250, valid_loss 1.9266283512115479\n",
            "iteration 1275, valid_loss 1.9299705028533936\n",
            "iteration 1300, valid_loss 2.5901613235473633\n",
            "iteration 1325, valid_loss 2.95868182182312\n",
            "iteration 1350, valid_loss 3.067495346069336\n",
            "iteration 1375, valid_loss 2.5712175369262695\n",
            "iteration 1400, valid_loss 2.953939199447632\n",
            "iteration 1425, valid_loss 3.2160186767578125\n",
            "iteration 1450, valid_loss 3.634053945541382\n",
            "iteration 1475, valid_loss 2.928335189819336\n",
            "iteration 1500, valid_loss 3.0992591381073\n",
            "iteration 1525, valid_loss 2.254164695739746\n",
            "iteration 1550, valid_loss 2.2943105697631836\n",
            "iteration 1575, valid_loss 2.866454601287842\n",
            "iteration 1600, valid_loss 2.304387331008911\n",
            "iteration 1625, valid_loss 3.1004199981689453\n",
            "iteration 1650, valid_loss 3.01534104347229\n",
            "iteration 1675, valid_loss 2.0998916625976562\n",
            "iteration 1700, valid_loss 2.012798309326172\n",
            "iteration 1725, valid_loss 3.8367204666137695\n",
            "iteration 1750, valid_loss 2.0067663192749023\n",
            "iteration 1775, valid_loss 3.013056516647339\n",
            "iteration 1800, valid_loss 2.3471992015838623\n",
            "iteration 1825, valid_loss 2.0781571865081787\n",
            "iteration 1850, valid_loss 4.307858943939209\n",
            "iteration 1875, valid_loss 4.1708984375\n",
            "iteration 1900, valid_loss 3.6878552436828613\n",
            "iteration 1925, valid_loss 1.801835536956787\n",
            "iteration 1950, valid_loss 2.183863639831543\n",
            "iteration 1975, valid_loss 2.4482221603393555\n",
            "iteration 2000, valid_loss 2.5541911125183105\n",
            "iteration 2025, valid_loss 1.98521888256073\n",
            "iteration 2050, valid_loss 3.424412250518799\n",
            "iteration 2075, valid_loss 2.360644578933716\n",
            "iteration 2100, valid_loss 2.3972537517547607\n",
            "iteration 2125, valid_loss 3.0836169719696045\n",
            "iteration 2150, valid_loss 3.5007832050323486\n",
            "iteration 2175, valid_loss 2.756679058074951\n",
            "iteration 2200, valid_loss 2.9900965690612793\n",
            "iteration 2225, valid_loss 2.957531452178955\n",
            "iteration 2250, valid_loss 3.795639753341675\n",
            "iteration 2275, valid_loss 2.8799405097961426\n",
            "iteration 2300, valid_loss 2.195335865020752\n",
            "iteration 2325, valid_loss 2.5802109241485596\n",
            "iteration 2350, valid_loss 2.175407886505127\n",
            "iteration 2375, valid_loss 2.897721767425537\n",
            "iteration 2400, valid_loss 2.6289734840393066\n",
            "iteration 2425, valid_loss 2.9137086868286133\n",
            "iteration 2450, valid_loss 2.6448795795440674\n",
            "iteration 2475, valid_loss 2.7658252716064453\n",
            "iteration 2500, valid_loss 2.391178607940674\n",
            "iteration 2525, valid_loss 3.221301317214966\n",
            "iteration 2550, valid_loss 3.657066583633423\n",
            "iteration 2575, valid_loss 3.201998472213745\n",
            "iteration 2600, valid_loss 3.5091912746429443\n",
            "iteration 2625, valid_loss 3.449317216873169\n",
            "iteration 2650, valid_loss 2.728029727935791\n",
            "iteration 2675, valid_loss 2.675633668899536\n",
            "iteration 2700, valid_loss 3.1375362873077393\n",
            "iteration 2725, valid_loss 3.327368974685669\n",
            "iteration 2750, valid_loss 3.6962952613830566\n",
            "iteration 2775, valid_loss 3.663957118988037\n",
            "iteration 2800, valid_loss 2.6729023456573486\n",
            "iteration 2825, valid_loss 2.7243692874908447\n",
            "iteration 2850, valid_loss 2.5201003551483154\n",
            "iteration 2875, valid_loss 2.9835379123687744\n",
            "iteration 2900, valid_loss 3.0742955207824707\n",
            "iteration 2925, valid_loss 3.0877974033355713\n",
            "iteration 2950, valid_loss 1.969316005706787\n",
            "iteration 2975, valid_loss 4.326987266540527\n",
            "iteration 3000, valid_loss 2.9305851459503174\n",
            "iteration 3025, valid_loss 3.1445353031158447\n",
            "iteration 3050, valid_loss 3.16442608833313\n",
            "iteration 3075, valid_loss 2.8943705558776855\n",
            "iteration 3100, valid_loss 3.434363603591919\n",
            "iteration 3125, valid_loss 2.5722126960754395\n",
            "iteration 3150, valid_loss 2.9195632934570312\n",
            "iteration 3175, valid_loss 2.9919142723083496\n",
            "iteration 3200, valid_loss 3.4506635665893555\n",
            "iteration 3225, valid_loss 3.6092112064361572\n",
            "iteration 3250, valid_loss 2.255411148071289\n",
            "iteration 3275, valid_loss 2.5332233905792236\n",
            "iteration 3300, valid_loss 4.640846252441406\n",
            "iteration 3325, valid_loss 2.039191961288452\n",
            "iteration 3350, valid_loss 3.225565195083618\n",
            "iteration 3375, valid_loss 2.036297559738159\n",
            "iteration 3400, valid_loss 2.695423126220703\n",
            "iteration 3425, valid_loss 2.4716596603393555\n",
            "iteration 3450, valid_loss 2.148284912109375\n",
            "iteration 3475, valid_loss 3.271129846572876\n",
            "iteration 3500, valid_loss 3.577345132827759\n",
            "iteration 3525, valid_loss 2.316206455230713\n",
            "iteration 3550, valid_loss 2.55877685546875\n",
            "iteration 3575, valid_loss 3.2474489212036133\n",
            "iteration 3600, valid_loss 2.07891583442688\n",
            "iteration 3625, valid_loss 3.111442804336548\n",
            "iteration 3650, valid_loss 3.4222002029418945\n",
            "iteration 3675, valid_loss 2.62101411819458\n",
            "iteration 3700, valid_loss 3.6822919845581055\n",
            "iteration 3725, valid_loss 2.497175455093384\n",
            "iteration 3750, valid_loss 2.715245008468628\n",
            "iteration 3775, valid_loss 3.133964776992798\n",
            "iteration 3800, valid_loss 2.925963878631592\n",
            "iteration 3825, valid_loss 3.076706886291504\n",
            "iteration 3850, valid_loss 2.6764626502990723\n",
            "iteration 3875, valid_loss 3.6702609062194824\n",
            "iteration 3900, valid_loss 2.0684118270874023\n",
            "iteration 3925, valid_loss 3.9016060829162598\n",
            "iteration 3950, valid_loss 3.6651854515075684\n",
            "iteration 3975, valid_loss 3.4393606185913086\n",
            "iteration 4000, valid_loss 2.4521825313568115\n",
            "iteration 4025, valid_loss 4.163149833679199\n",
            "iteration 4050, valid_loss 3.2538673877716064\n",
            "iteration 4075, valid_loss 2.568143606185913\n",
            "iteration 4100, valid_loss 3.3057641983032227\n",
            "iteration 4125, valid_loss 2.4026265144348145\n",
            "iteration 4150, valid_loss 2.6437442302703857\n",
            "iteration 4175, valid_loss 3.8432116508483887\n",
            "iteration 4200, valid_loss 1.7740598917007446\n",
            "iteration 4225, valid_loss 2.4779398441314697\n",
            "iteration 4250, valid_loss 3.030923366546631\n",
            "iteration 4275, valid_loss 2.2508883476257324\n",
            "iteration 4300, valid_loss 2.7771263122558594\n",
            "iteration 4325, valid_loss 2.1615207195281982\n",
            "iteration 4350, valid_loss 3.3628931045532227\n",
            "iteration 4375, valid_loss 2.82765531539917\n",
            "iteration 4400, valid_loss 3.1857352256774902\n",
            "iteration 4425, valid_loss 4.753629207611084\n",
            "iteration 4450, valid_loss 2.210939407348633\n",
            "iteration 4475, valid_loss 2.912248134613037\n",
            "iteration 4500, valid_loss 2.8501029014587402\n",
            "iteration 4525, valid_loss 3.1025424003601074\n",
            "iteration 4550, valid_loss 4.931349754333496\n",
            "iteration 4575, valid_loss 3.4346156120300293\n",
            "iteration 4600, valid_loss 3.441725254058838\n",
            "iteration 4625, valid_loss 2.5570216178894043\n",
            "iteration 4650, valid_loss 3.8022403717041016\n",
            "iteration 4675, valid_loss 2.872331380844116\n",
            "iteration 4700, valid_loss 2.483933925628662\n",
            "iteration 4725, valid_loss 3.0636541843414307\n",
            "iteration 4750, valid_loss 3.8815221786499023\n",
            "iteration 4775, valid_loss 4.4216837882995605\n",
            "iteration 4800, valid_loss 3.175694704055786\n",
            "iteration 4825, valid_loss 4.0717997550964355\n",
            "iteration 4850, valid_loss 3.3511712551116943\n",
            "iteration 4875, valid_loss 3.5376384258270264\n",
            "iteration 4900, valid_loss 3.9180850982666016\n",
            "iteration 4925, valid_loss 2.888364791870117\n",
            "iteration 4950, valid_loss 3.44572114944458\n",
            "iteration 4975, valid_loss 3.1535096168518066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLGZN8Rlu79",
        "outputId": "9e0f77ae-7bbd-4e2e-dcb5-4b8a1aa7fa77"
      },
      "source": [
        "f1_score(outs_to_f1, targets_to_f1, average='micro')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.61984375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u62voccTnLbY",
        "outputId": "b92b17b7-fac3-44d9-9214-4f22c05b8868"
      },
      "source": [
        "print('Результат ухудшился :(')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Результат ухудшился :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbMGTvwcn_1U"
      },
      "source": [
        "Теперь ReduceLROnPlateau + ставим легендарный лёрнинг рейт Карпатого"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwPpo4uNeZKC",
        "outputId": "fa55565e-8134-4bc7-a483-e0483ab6715e"
      },
      "source": [
        "optimizer = Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad], betas=(0.9, 0.999), eps=1e-5, lr=3e-4\n",
        ")\n",
        "\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "x = []\n",
        "outs_to_f1 = np.array(x)\n",
        "\n",
        "x = []\n",
        "targets_to_f1 = np.array(x)\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input, target = trainset.next_batch(BATCH_SIZE, device=device)\n",
        "    out = model(input)\n",
        "    loss = criterion(out, target.long())\n",
        "    loss.backward()\n",
        "    scheduler.step(loss)\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 25 == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input, target = validset.next_batch(BATCH_SIZE, device=device)\n",
        "            out = model(input)\n",
        "            out_ = m(out)\n",
        "            out_ = torch.argmax(out_, dim=1)\n",
        "            valid_loss =  criterion(out, target.long())\n",
        "            out, target = out_.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
        "\n",
        "            outs_to_f1 = np.concatenate((outs_to_f1, out), axis=None)\n",
        "            targets_to_f1 = np.concatenate((targets_to_f1, target), axis=None) # для проверки качества\n",
        "\n",
        "            print(\"iteration {}, valid_loss {}\".format(i, valid_loss.item()))\n",
        "\n",
        "        model.train()\n",
        "\n",
        "f1_score(outs_to_f1_test, targets_to_f1_test, average='micro')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0, valid_loss 2.0714356899261475\n",
            "iteration 25, valid_loss 2.533285617828369\n",
            "iteration 50, valid_loss 4.399380683898926\n",
            "iteration 75, valid_loss 3.5106887817382812\n",
            "iteration 100, valid_loss 4.764191627502441\n",
            "iteration 125, valid_loss 3.0673229694366455\n",
            "iteration 150, valid_loss 1.6830432415008545\n",
            "iteration 175, valid_loss 2.6382737159729004\n",
            "iteration 200, valid_loss 3.2412405014038086\n",
            "iteration 225, valid_loss 2.2630040645599365\n",
            "iteration 250, valid_loss 1.904585599899292\n",
            "iteration 275, valid_loss 3.5669631958007812\n",
            "iteration 300, valid_loss 2.959951639175415\n",
            "iteration 325, valid_loss 2.348383665084839\n",
            "iteration 350, valid_loss 3.047560691833496\n",
            "iteration 375, valid_loss 2.9528329372406006\n",
            "iteration 400, valid_loss 3.292499542236328\n",
            "iteration 425, valid_loss 3.5191667079925537\n",
            "iteration 450, valid_loss 3.7114524841308594\n",
            "iteration 475, valid_loss 3.3247110843658447\n",
            "iteration 500, valid_loss 4.385577201843262\n",
            "iteration 525, valid_loss 3.068284273147583\n",
            "iteration 550, valid_loss 3.0931813716888428\n",
            "iteration 575, valid_loss 3.103877305984497\n",
            "iteration 600, valid_loss 2.9678359031677246\n",
            "iteration 625, valid_loss 3.1554713249206543\n",
            "iteration 650, valid_loss 2.390434503555298\n",
            "iteration 675, valid_loss 3.343123197555542\n",
            "iteration 700, valid_loss 2.7968263626098633\n",
            "iteration 725, valid_loss 1.8072303533554077\n",
            "iteration 750, valid_loss 3.8749327659606934\n",
            "iteration 775, valid_loss 3.1256134510040283\n",
            "iteration 800, valid_loss 1.9711158275604248\n",
            "iteration 825, valid_loss 2.4690022468566895\n",
            "iteration 850, valid_loss 2.1220407485961914\n",
            "iteration 875, valid_loss 3.4322891235351562\n",
            "iteration 900, valid_loss 3.6773593425750732\n",
            "iteration 925, valid_loss 2.355229616165161\n",
            "iteration 950, valid_loss 2.018805980682373\n",
            "iteration 975, valid_loss 2.7462689876556396\n",
            "iteration 1000, valid_loss 3.3109147548675537\n",
            "iteration 1025, valid_loss 1.955986499786377\n",
            "iteration 1050, valid_loss 3.18649959564209\n",
            "iteration 1075, valid_loss 1.8640649318695068\n",
            "iteration 1100, valid_loss 2.607844829559326\n",
            "iteration 1125, valid_loss 2.929189920425415\n",
            "iteration 1150, valid_loss 2.60133695602417\n",
            "iteration 1175, valid_loss 3.7553205490112305\n",
            "iteration 1200, valid_loss 2.594635486602783\n",
            "iteration 1225, valid_loss 2.3695600032806396\n",
            "iteration 1250, valid_loss 1.8898245096206665\n",
            "iteration 1275, valid_loss 3.1784141063690186\n",
            "iteration 1300, valid_loss 3.0775716304779053\n",
            "iteration 1325, valid_loss 2.4965152740478516\n",
            "iteration 1350, valid_loss 2.881442070007324\n",
            "iteration 1375, valid_loss 2.9289932250976562\n",
            "iteration 1400, valid_loss 3.3240864276885986\n",
            "iteration 1425, valid_loss 2.8134775161743164\n",
            "iteration 1450, valid_loss 2.5751051902770996\n",
            "iteration 1475, valid_loss 3.464718818664551\n",
            "iteration 1500, valid_loss 2.41375732421875\n",
            "iteration 1525, valid_loss 3.2966678142547607\n",
            "iteration 1550, valid_loss 2.9637115001678467\n",
            "iteration 1575, valid_loss 2.63645076751709\n",
            "iteration 1600, valid_loss 3.895184278488159\n",
            "iteration 1625, valid_loss 3.5150270462036133\n",
            "iteration 1650, valid_loss 4.271612167358398\n",
            "iteration 1675, valid_loss 2.6177124977111816\n",
            "iteration 1700, valid_loss 2.359318971633911\n",
            "iteration 1725, valid_loss 2.4491825103759766\n",
            "iteration 1750, valid_loss 3.148862361907959\n",
            "iteration 1775, valid_loss 2.344127655029297\n",
            "iteration 1800, valid_loss 2.849182605743408\n",
            "iteration 1825, valid_loss 2.406405448913574\n",
            "iteration 1850, valid_loss 2.5971553325653076\n",
            "iteration 1875, valid_loss 2.019496440887451\n",
            "iteration 1900, valid_loss 2.958738327026367\n",
            "iteration 1925, valid_loss 2.150444984436035\n",
            "iteration 1950, valid_loss 2.475978374481201\n",
            "iteration 1975, valid_loss 2.3481719493865967\n",
            "iteration 2000, valid_loss 2.7529499530792236\n",
            "iteration 2025, valid_loss 2.4367518424987793\n",
            "iteration 2050, valid_loss 3.2990164756774902\n",
            "iteration 2075, valid_loss 2.4814298152923584\n",
            "iteration 2100, valid_loss 2.322066068649292\n",
            "iteration 2125, valid_loss 3.0938737392425537\n",
            "iteration 2150, valid_loss 2.721362829208374\n",
            "iteration 2175, valid_loss 3.05051851272583\n",
            "iteration 2200, valid_loss 2.6337366104125977\n",
            "iteration 2225, valid_loss 4.159698963165283\n",
            "iteration 2250, valid_loss 2.481203317642212\n",
            "iteration 2275, valid_loss 3.558655261993408\n",
            "iteration 2300, valid_loss 2.827329635620117\n",
            "iteration 2325, valid_loss 2.6179134845733643\n",
            "iteration 2350, valid_loss 4.9452691078186035\n",
            "iteration 2375, valid_loss 3.204220771789551\n",
            "iteration 2400, valid_loss 2.716209888458252\n",
            "iteration 2425, valid_loss 3.0424628257751465\n",
            "iteration 2450, valid_loss 2.5152273178100586\n",
            "iteration 2475, valid_loss 4.6442437171936035\n",
            "iteration 2500, valid_loss 1.895690679550171\n",
            "iteration 2525, valid_loss 1.5725758075714111\n",
            "iteration 2550, valid_loss 1.3232296705245972\n",
            "iteration 2575, valid_loss 2.4335622787475586\n",
            "iteration 2600, valid_loss 1.9051682949066162\n",
            "iteration 2625, valid_loss 4.191676616668701\n",
            "iteration 2650, valid_loss 4.216184616088867\n",
            "iteration 2675, valid_loss 3.4426779747009277\n",
            "iteration 2700, valid_loss 2.3253700733184814\n",
            "iteration 2725, valid_loss 2.422417163848877\n",
            "iteration 2750, valid_loss 3.2440783977508545\n",
            "iteration 2775, valid_loss 3.1401124000549316\n",
            "iteration 2800, valid_loss 2.7123751640319824\n",
            "iteration 2825, valid_loss 1.7628179788589478\n",
            "iteration 2850, valid_loss 3.8992538452148438\n",
            "iteration 2875, valid_loss 2.7604033946990967\n",
            "iteration 2900, valid_loss 1.893484115600586\n",
            "iteration 2925, valid_loss 2.727890968322754\n",
            "iteration 2950, valid_loss 3.5097107887268066\n",
            "iteration 2975, valid_loss 3.2148587703704834\n",
            "iteration 3000, valid_loss 2.0292134284973145\n",
            "iteration 3025, valid_loss 4.389029026031494\n",
            "iteration 3050, valid_loss 3.1373507976531982\n",
            "iteration 3075, valid_loss 2.453808069229126\n",
            "iteration 3100, valid_loss 3.952735185623169\n",
            "iteration 3125, valid_loss 3.141385316848755\n",
            "iteration 3150, valid_loss 2.8847429752349854\n",
            "iteration 3175, valid_loss 3.776376962661743\n",
            "iteration 3200, valid_loss 2.747753143310547\n",
            "iteration 3225, valid_loss 2.851686716079712\n",
            "iteration 3250, valid_loss 3.2800357341766357\n",
            "iteration 3275, valid_loss 1.9170209169387817\n",
            "iteration 3300, valid_loss 3.4202420711517334\n",
            "iteration 3325, valid_loss 3.4004955291748047\n",
            "iteration 3350, valid_loss 2.738821029663086\n",
            "iteration 3375, valid_loss 2.4639194011688232\n",
            "iteration 3400, valid_loss 2.0350866317749023\n",
            "iteration 3425, valid_loss 3.167738676071167\n",
            "iteration 3450, valid_loss 2.566631317138672\n",
            "iteration 3475, valid_loss 3.4782309532165527\n",
            "iteration 3500, valid_loss 3.0579640865325928\n",
            "iteration 3525, valid_loss 3.6505916118621826\n",
            "iteration 3550, valid_loss 3.4889581203460693\n",
            "iteration 3575, valid_loss 2.290430784225464\n",
            "iteration 3600, valid_loss 2.546931266784668\n",
            "iteration 3625, valid_loss 2.1256392002105713\n",
            "iteration 3650, valid_loss 1.5225332975387573\n",
            "iteration 3675, valid_loss 2.2848799228668213\n",
            "iteration 3700, valid_loss 3.7113912105560303\n",
            "iteration 3725, valid_loss 3.0644397735595703\n",
            "iteration 3750, valid_loss 3.068761110305786\n",
            "iteration 3775, valid_loss 3.0177152156829834\n",
            "iteration 3800, valid_loss 2.3327386379241943\n",
            "iteration 3825, valid_loss 3.1726419925689697\n",
            "iteration 3850, valid_loss 4.098743438720703\n",
            "iteration 3875, valid_loss 3.4615869522094727\n",
            "iteration 3900, valid_loss 2.522247791290283\n",
            "iteration 3925, valid_loss 3.3220338821411133\n",
            "iteration 3950, valid_loss 3.272674083709717\n",
            "iteration 3975, valid_loss 3.6353678703308105\n",
            "iteration 4000, valid_loss 1.3797510862350464\n",
            "iteration 4025, valid_loss 2.5911450386047363\n",
            "iteration 4050, valid_loss 3.8383636474609375\n",
            "iteration 4075, valid_loss 2.4541358947753906\n",
            "iteration 4100, valid_loss 3.196763277053833\n",
            "iteration 4125, valid_loss 2.566751003265381\n",
            "iteration 4150, valid_loss 2.873169183731079\n",
            "iteration 4175, valid_loss 2.639854669570923\n",
            "iteration 4200, valid_loss 3.018925666809082\n",
            "iteration 4225, valid_loss 4.768304824829102\n",
            "iteration 4250, valid_loss 3.571810007095337\n",
            "iteration 4275, valid_loss 2.1198573112487793\n",
            "iteration 4300, valid_loss 2.7664666175842285\n",
            "iteration 4325, valid_loss 3.7875146865844727\n",
            "iteration 4350, valid_loss 2.9803965091705322\n",
            "iteration 4375, valid_loss 3.1494829654693604\n",
            "iteration 4400, valid_loss 3.160465717315674\n",
            "iteration 4425, valid_loss 3.0891294479370117\n",
            "iteration 4450, valid_loss 3.213590383529663\n",
            "iteration 4475, valid_loss 3.461892604827881\n",
            "iteration 4500, valid_loss 3.181749105453491\n",
            "iteration 4525, valid_loss 3.72244930267334\n",
            "iteration 4550, valid_loss 2.766913890838623\n",
            "iteration 4575, valid_loss 1.920393705368042\n",
            "iteration 4600, valid_loss 2.5379836559295654\n",
            "iteration 4625, valid_loss 2.548161029815674\n",
            "iteration 4650, valid_loss 2.2975144386291504\n",
            "iteration 4675, valid_loss 2.7473349571228027\n",
            "iteration 4700, valid_loss 1.626822829246521\n",
            "iteration 4725, valid_loss 3.3798062801361084\n",
            "iteration 4750, valid_loss 2.4542949199676514\n",
            "iteration 4775, valid_loss 2.243027448654175\n",
            "iteration 4800, valid_loss 1.8371108770370483\n",
            "iteration 4825, valid_loss 2.709622859954834\n",
            "iteration 4850, valid_loss 3.484877586364746\n",
            "iteration 4875, valid_loss 4.036585330963135\n",
            "iteration 4900, valid_loss 3.3794355392456055\n",
            "iteration 4925, valid_loss 1.8360108137130737\n",
            "iteration 4950, valid_loss 2.8733036518096924\n",
            "iteration 4975, valid_loss 2.4832093715667725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.63118125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR_4JRDjjLkn",
        "outputId": "d7ade11c-be57-45e0-8a1e-8ed2b9579d14"
      },
      "source": [
        "f1_score(outs_to_f1, targets_to_f1, average='micro')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.618125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umxr1dJ56Ddh",
        "outputId": "c5fa4d74-45fc-42d7-ffdd-f97c114be3fb"
      },
      "source": [
        "print('Результат ухудшился :(')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Результат ухудшился :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARafhtPVGJf1"
      },
      "source": [
        "Попробуем lr Карпатого без шедулеров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQJ57F8WGCcx",
        "outputId": "5e47adf6-b34d-4458-b36f-929b3f0b16b7"
      },
      "source": [
        "optimizer = Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad], betas=(0.9, 0.999), eps=1e-5, lr=3e-4\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "x = []\n",
        "outs_to_f1 = np.array(x)\n",
        "\n",
        "x = []\n",
        "targets_to_f1 = np.array(x)\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input, target = trainset.next_batch(BATCH_SIZE, device=device)\n",
        "    out = model(input)\n",
        "    loss = criterion(out, target.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 25 == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input, target = validset.next_batch(BATCH_SIZE, device=device)\n",
        "            out = model(input)\n",
        "            out_ = m(out)\n",
        "            out_ = torch.argmax(out_, dim=1)\n",
        "            valid_loss =  criterion(out, target.long())\n",
        "            out, target = out_.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
        "\n",
        "            outs_to_f1 = np.concatenate((outs_to_f1, out), axis=None)\n",
        "            targets_to_f1 = np.concatenate((targets_to_f1, target), axis=None) # для проверки качества\n",
        "\n",
        "            print(\"iteration {}, valid_loss {}\".format(i, valid_loss.item()))\n",
        "\n",
        "        model.train()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0, valid_loss 2.850710868835449\n",
            "iteration 25, valid_loss 2.884730339050293\n",
            "iteration 50, valid_loss 2.7052457332611084\n",
            "iteration 75, valid_loss 1.9372246265411377\n",
            "iteration 100, valid_loss 2.2819437980651855\n",
            "iteration 125, valid_loss 1.7909961938858032\n",
            "iteration 150, valid_loss 2.7792162895202637\n",
            "iteration 175, valid_loss 2.276474714279175\n",
            "iteration 200, valid_loss 2.5383028984069824\n",
            "iteration 225, valid_loss 2.7397470474243164\n",
            "iteration 250, valid_loss 4.543215274810791\n",
            "iteration 275, valid_loss 2.607646942138672\n",
            "iteration 300, valid_loss 2.5234498977661133\n",
            "iteration 325, valid_loss 1.8086931705474854\n",
            "iteration 350, valid_loss 2.4767892360687256\n",
            "iteration 375, valid_loss 3.6899139881134033\n",
            "iteration 400, valid_loss 2.744711399078369\n",
            "iteration 425, valid_loss 2.656219720840454\n",
            "iteration 450, valid_loss 3.3124282360076904\n",
            "iteration 475, valid_loss 3.3163321018218994\n",
            "iteration 500, valid_loss 1.364060640335083\n",
            "iteration 525, valid_loss 4.3191752433776855\n",
            "iteration 550, valid_loss 3.704254627227783\n",
            "iteration 575, valid_loss 3.391751766204834\n",
            "iteration 600, valid_loss 2.637500047683716\n",
            "iteration 625, valid_loss 3.942694664001465\n",
            "iteration 650, valid_loss 2.273607015609741\n",
            "iteration 675, valid_loss 3.0816915035247803\n",
            "iteration 700, valid_loss 3.626951217651367\n",
            "iteration 725, valid_loss 1.9893537759780884\n",
            "iteration 750, valid_loss 4.160228729248047\n",
            "iteration 775, valid_loss 3.502859592437744\n",
            "iteration 800, valid_loss 2.3039352893829346\n",
            "iteration 825, valid_loss 1.9463753700256348\n",
            "iteration 850, valid_loss 3.2127158641815186\n",
            "iteration 875, valid_loss 3.6968095302581787\n",
            "iteration 900, valid_loss 3.7339422702789307\n",
            "iteration 925, valid_loss 2.117335081100464\n",
            "iteration 950, valid_loss 1.773600459098816\n",
            "iteration 975, valid_loss 2.3107855319976807\n",
            "iteration 1000, valid_loss 2.630025625228882\n",
            "iteration 1025, valid_loss 2.4805381298065186\n",
            "iteration 1050, valid_loss 3.4853875637054443\n",
            "iteration 1075, valid_loss 3.374227285385132\n",
            "iteration 1100, valid_loss 2.7739953994750977\n",
            "iteration 1125, valid_loss 2.804969310760498\n",
            "iteration 1150, valid_loss 3.854735851287842\n",
            "iteration 1175, valid_loss 2.9796640872955322\n",
            "iteration 1200, valid_loss 3.184157609939575\n",
            "iteration 1225, valid_loss 1.414906620979309\n",
            "iteration 1250, valid_loss 2.4006459712982178\n",
            "iteration 1275, valid_loss 2.4282822608947754\n",
            "iteration 1300, valid_loss 3.0715785026550293\n",
            "iteration 1325, valid_loss 2.915576457977295\n",
            "iteration 1350, valid_loss 4.012243747711182\n",
            "iteration 1375, valid_loss 3.0057945251464844\n",
            "iteration 1400, valid_loss 2.7131829261779785\n",
            "iteration 1425, valid_loss 3.1196274757385254\n",
            "iteration 1450, valid_loss 3.3513975143432617\n",
            "iteration 1475, valid_loss 3.538088321685791\n",
            "iteration 1500, valid_loss 3.898401975631714\n",
            "iteration 1525, valid_loss 3.497300386428833\n",
            "iteration 1550, valid_loss 3.309387683868408\n",
            "iteration 1575, valid_loss 4.3159308433532715\n",
            "iteration 1600, valid_loss 3.0134315490722656\n",
            "iteration 1625, valid_loss 2.8872718811035156\n",
            "iteration 1650, valid_loss 2.3980462551116943\n",
            "iteration 1675, valid_loss 3.399733066558838\n",
            "iteration 1700, valid_loss 4.415499210357666\n",
            "iteration 1725, valid_loss 2.6240532398223877\n",
            "iteration 1750, valid_loss 2.686671495437622\n",
            "iteration 1775, valid_loss 1.274061918258667\n",
            "iteration 1800, valid_loss 4.248556137084961\n",
            "iteration 1825, valid_loss 3.576538324356079\n",
            "iteration 1850, valid_loss 3.4030189514160156\n",
            "iteration 1875, valid_loss 3.2561681270599365\n",
            "iteration 1900, valid_loss 3.0572025775909424\n",
            "iteration 1925, valid_loss 2.8498268127441406\n",
            "iteration 1950, valid_loss 3.111884593963623\n",
            "iteration 1975, valid_loss 3.36395525932312\n",
            "iteration 2000, valid_loss 3.9978508949279785\n",
            "iteration 2025, valid_loss 3.62719464302063\n",
            "iteration 2050, valid_loss 3.7351346015930176\n",
            "iteration 2075, valid_loss 5.605504512786865\n",
            "iteration 2100, valid_loss 2.8081889152526855\n",
            "iteration 2125, valid_loss 3.166613817214966\n",
            "iteration 2150, valid_loss 2.5755715370178223\n",
            "iteration 2175, valid_loss 3.1962547302246094\n",
            "iteration 2200, valid_loss 4.693636894226074\n",
            "iteration 2225, valid_loss 3.72540545463562\n",
            "iteration 2250, valid_loss 3.338378429412842\n",
            "iteration 2275, valid_loss 3.4870245456695557\n",
            "iteration 2300, valid_loss 3.5350453853607178\n",
            "iteration 2325, valid_loss 2.1873741149902344\n",
            "iteration 2350, valid_loss 4.074464797973633\n",
            "iteration 2375, valid_loss 5.724156856536865\n",
            "iteration 2400, valid_loss 2.740778923034668\n",
            "iteration 2425, valid_loss 3.6146950721740723\n",
            "iteration 2450, valid_loss 2.534721612930298\n",
            "iteration 2475, valid_loss 2.9606523513793945\n",
            "iteration 2500, valid_loss 3.226168155670166\n",
            "iteration 2525, valid_loss 3.0892226696014404\n",
            "iteration 2550, valid_loss 3.4072189331054688\n",
            "iteration 2575, valid_loss 3.951551914215088\n",
            "iteration 2600, valid_loss 3.5402040481567383\n",
            "iteration 2625, valid_loss 3.269291639328003\n",
            "iteration 2650, valid_loss 3.273125410079956\n",
            "iteration 2675, valid_loss 3.636847734451294\n",
            "iteration 2700, valid_loss 2.352814197540283\n",
            "iteration 2725, valid_loss 2.134232521057129\n",
            "iteration 2750, valid_loss 3.503380298614502\n",
            "iteration 2775, valid_loss 2.8909213542938232\n",
            "iteration 2800, valid_loss 3.7652206420898438\n",
            "iteration 2825, valid_loss 1.5777088403701782\n",
            "iteration 2850, valid_loss 3.8506603240966797\n",
            "iteration 2875, valid_loss 3.907352924346924\n",
            "iteration 2900, valid_loss 4.716978073120117\n",
            "iteration 2925, valid_loss 3.308457374572754\n",
            "iteration 2950, valid_loss 3.2604193687438965\n",
            "iteration 2975, valid_loss 4.610095500946045\n",
            "iteration 3000, valid_loss 4.108913421630859\n",
            "iteration 3025, valid_loss 3.008208990097046\n",
            "iteration 3050, valid_loss 4.708709239959717\n",
            "iteration 3075, valid_loss 3.408860921859741\n",
            "iteration 3100, valid_loss 3.9594388008117676\n",
            "iteration 3125, valid_loss 3.990647554397583\n",
            "iteration 3150, valid_loss 4.662903308868408\n",
            "iteration 3175, valid_loss 2.921356201171875\n",
            "iteration 3200, valid_loss 4.0516862869262695\n",
            "iteration 3225, valid_loss 2.9254109859466553\n",
            "iteration 3250, valid_loss 2.4584031105041504\n",
            "iteration 3275, valid_loss 3.5643999576568604\n",
            "iteration 3300, valid_loss 2.311414957046509\n",
            "iteration 3325, valid_loss 3.2139010429382324\n",
            "iteration 3350, valid_loss 3.016561269760132\n",
            "iteration 3375, valid_loss 3.385108232498169\n",
            "iteration 3400, valid_loss 2.9777636528015137\n",
            "iteration 3425, valid_loss 2.3772084712982178\n",
            "iteration 3450, valid_loss 2.9108333587646484\n",
            "iteration 3475, valid_loss 4.092186450958252\n",
            "iteration 3500, valid_loss 3.3550384044647217\n",
            "iteration 3525, valid_loss 3.9610533714294434\n",
            "iteration 3550, valid_loss 3.124807357788086\n",
            "iteration 3575, valid_loss 2.499730110168457\n",
            "iteration 3600, valid_loss 3.8650271892547607\n",
            "iteration 3625, valid_loss 3.5827016830444336\n",
            "iteration 3650, valid_loss 2.45094895362854\n",
            "iteration 3675, valid_loss 5.082137107849121\n",
            "iteration 3700, valid_loss 3.849074125289917\n",
            "iteration 3725, valid_loss 3.846696615219116\n",
            "iteration 3750, valid_loss 2.9116501808166504\n",
            "iteration 3775, valid_loss 3.7763497829437256\n",
            "iteration 3800, valid_loss 3.922473430633545\n",
            "iteration 3825, valid_loss 2.493802309036255\n",
            "iteration 3850, valid_loss 3.543311595916748\n",
            "iteration 3875, valid_loss 3.011169672012329\n",
            "iteration 3900, valid_loss 3.1936445236206055\n",
            "iteration 3925, valid_loss 4.3628010749816895\n",
            "iteration 3950, valid_loss 3.492837905883789\n",
            "iteration 3975, valid_loss 6.921604156494141\n",
            "iteration 4000, valid_loss 5.058770656585693\n",
            "iteration 4025, valid_loss 3.5731101036071777\n",
            "iteration 4050, valid_loss 3.160604476928711\n",
            "iteration 4075, valid_loss 4.058537483215332\n",
            "iteration 4100, valid_loss 3.734679937362671\n",
            "iteration 4125, valid_loss 2.5160186290740967\n",
            "iteration 4150, valid_loss 2.947722911834717\n",
            "iteration 4175, valid_loss 4.341089725494385\n",
            "iteration 4200, valid_loss 3.1465837955474854\n",
            "iteration 4225, valid_loss 4.313750267028809\n",
            "iteration 4250, valid_loss 2.444948434829712\n",
            "iteration 4275, valid_loss 3.576207160949707\n",
            "iteration 4300, valid_loss 3.9856655597686768\n",
            "iteration 4325, valid_loss 3.0679497718811035\n",
            "iteration 4350, valid_loss 3.430722236633301\n",
            "iteration 4375, valid_loss 3.244270086288452\n",
            "iteration 4400, valid_loss 5.307577133178711\n",
            "iteration 4425, valid_loss 3.553185224533081\n",
            "iteration 4450, valid_loss 2.704371929168701\n",
            "iteration 4475, valid_loss 4.599304676055908\n",
            "iteration 4500, valid_loss 3.885877847671509\n",
            "iteration 4525, valid_loss 3.1007370948791504\n",
            "iteration 4550, valid_loss 3.076362371444702\n",
            "iteration 4575, valid_loss 3.8741135597229004\n",
            "iteration 4600, valid_loss 2.366421699523926\n",
            "iteration 4625, valid_loss 3.1358466148376465\n",
            "iteration 4650, valid_loss 5.0365891456604\n",
            "iteration 4675, valid_loss 4.176248550415039\n",
            "iteration 4700, valid_loss 4.2020697593688965\n",
            "iteration 4725, valid_loss 3.378183126449585\n",
            "iteration 4750, valid_loss 3.564204216003418\n",
            "iteration 4775, valid_loss 3.7448720932006836\n",
            "iteration 4800, valid_loss 4.639703273773193\n",
            "iteration 4825, valid_loss 4.628751277923584\n",
            "iteration 4850, valid_loss 4.201672077178955\n",
            "iteration 4875, valid_loss 3.6655659675598145\n",
            "iteration 4900, valid_loss 5.364856243133545\n",
            "iteration 4925, valid_loss 3.628344774246216\n",
            "iteration 4950, valid_loss 4.09502649307251\n",
            "iteration 4975, valid_loss 4.179877281188965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUeUlV0tGRFv",
        "outputId": "b736352a-2b43-47c8-b3c5-e294403783a9"
      },
      "source": [
        "f1_score(outs_to_f1, targets_to_f1, average='micro')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.643984375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw4e4-2UP0y8"
      },
      "source": [
        "Результат чуть ухудшился всё-таки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8FEV9ppQIJ"
      },
      "source": [
        "Также, кроме обучения на более большом датасете, есть вариант аугментации данных. Конечно, как вариант сделать больше слоёв в сетке. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxx8bBHCQ906",
        "outputId": "e69736cb-e1da-4b54-d1d1-1c55a4fafdb5"
      },
      "source": [
        "np.unique(outs_to_f1_test, axis=0) # проверка что предсказывает все классы"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrHIJo8cUHR"
      },
      "source": [
        "Одна из проблем - это дизбаланс класса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIkMGZbOSLg4",
        "outputId": "3ca71f4f-d6d1-410b-bb88-f4be5b06414c"
      },
      "source": [
        "unique, counts = np.unique(outs_to_f1_test, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0: 35392, 1.0: 263883, 2.0: 20725}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhzzHUtYSOoG",
        "outputId": "7541ded4-0fa9-4c3b-8ec4-3178d5160271"
      },
      "source": [
        "unique, counts = np.unique(targets_to_f1_test, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0: 48999, 1.0: 230122, 2.0: 40879}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5veZiCzDSiAb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}