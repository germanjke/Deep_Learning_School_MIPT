{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homeworkTextSummarization_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6e25c0c8a3f44f2b52ddec78c53afe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42a1244a8d094d35912e063d147690b9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0fed38155ef473692c9a5db19e8ce90",
              "IPY_MODEL_44cc66dc62b64680a1d7f73460311f97"
            ]
          }
        },
        "42a1244a8d094d35912e063d147690b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0fed38155ef473692c9a5db19e8ce90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8aaf328e39d4fb1bade97a1070c0944",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78427412ef8c4f64a5cb4316fd9e51db"
          }
        },
        "44cc66dc62b64680a1d7f73460311f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_152f6c043c24407692134d38786fc661",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 1.72MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e80805cbb4543f691f8ebeab1cbc99c"
          }
        },
        "d8aaf328e39d4fb1bade97a1070c0944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78427412ef8c4f64a5cb4316fd9e51db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "152f6c043c24407692134d38786fc661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e80805cbb4543f691f8ebeab1cbc99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc293a8439b1451bad6cd6f74d2e52d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7928a138685e439cba0ff09f397d8523",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a73c7dc8446a4d398b82303deacfba5e",
              "IPY_MODEL_c61b9bf96be14d528ba6a9af218a3848"
            ]
          }
        },
        "7928a138685e439cba0ff09f397d8523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a73c7dc8446a4d398b82303deacfba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4419fde47c014b95ad96aff1ca0c508a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3a9a2d66ba14bc887e86e60a913a5c0"
          }
        },
        "c61b9bf96be14d528ba6a9af218a3848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e2d296265821476dbb03a4ae248ec9f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [01:50&lt;00:00,  9.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1a8d0dd7d5f4ff89fadc333f195a04a"
          }
        },
        "4419fde47c014b95ad96aff1ca0c508a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3a9a2d66ba14bc887e86e60a913a5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2d296265821476dbb03a4ae248ec9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1a8d0dd7d5f4ff89fadc333f195a04a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34d14220ad9546d7b96ff6de61a1e0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b785efc4d7974acba2b009716bfd2926",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1e502179f7714f4ca281f383bf40a4f3",
              "IPY_MODEL_fb1c6d9f0c5245d6b6f81d02cccffbb7"
            ]
          }
        },
        "b785efc4d7974acba2b009716bfd2926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e502179f7714f4ca281f383bf40a4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0441c107c25a40b8953dd1b1c766b2f8",
            "_dom_classes": [],
            "description": "Train Loss: 0.204, Valid Loss: 0.182:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 552,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5c5471fb385486c82324087a5a32f63"
          }
        },
        "fb1c6d9f0c5245d6b6f81d02cccffbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9f58f788cac476684847a7e32e37941",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/552 [00:00&lt;04:48,  1.92it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b57d4e937474ab49477efea9100fdb1"
          }
        },
        "0441c107c25a40b8953dd1b1c766b2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5c5471fb385486c82324087a5a32f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9f58f788cac476684847a7e32e37941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b57d4e937474ab49477efea9100fdb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f4d1c4c29f14ba487f674034c949676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_042caa39ec554633a21d284db09fcc8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_976cdf106994467e9859d2e05fc9a304",
              "IPY_MODEL_d5c8f1c15704469d91b06299aabb501f"
            ]
          }
        },
        "042caa39ec554633a21d284db09fcc8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "976cdf106994467e9859d2e05fc9a304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fc1009a6a8e14b66a9cf2e6f2ff712e0",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fb66e6a9b284483bf9c4fe15cb92f85"
          }
        },
        "d5c8f1c15704469d91b06299aabb501f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6cf1acb7d65e4f1e95fbc1bc725acf6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [06:24&lt;00:00, 384.67s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a6b6d840b1645f28e1f1541299ab1d2"
          }
        },
        "fc1009a6a8e14b66a9cf2e6f2ff712e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fb66e6a9b284483bf9c4fe15cb92f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cf1acb7d65e4f1e95fbc1bc725acf6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a6b6d840b1645f28e1f1541299ab1d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/germanjke/Deep_Learning_School_MIPT/blob/master/nlp_homeworks/homeworkTextSummarization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmb8UhIzOnfK"
      },
      "source": [
        "# Text Summarization. Homework\n",
        "\n",
        "Всем привет! Это домашка по суммаризации текста.\n",
        "\n",
        "На семинаре мы рассмотрели базовые модели для суммаризации текста. Попробуйте теперь улучшить два метода: TextRank и Extractive RNN. Задание достаточно большое и требует хорошую фантазию, тут можно эксперементировать во всю.\n",
        "\n",
        "Для сдачи заданий надо получить определенное качество по test-у:\n",
        "\n",
        "- 1 задание: 0.35 BLEU\n",
        "- 2 задание: 0.35 BLEU\n",
        "\n",
        "Если ваш подход пробивает это качество – задание считается пройденным. Плюсом будет описание того, почему вы решили использовать то или иное решение. \n",
        "\n",
        "Датасет: gazeta.ru\n",
        "\n",
        "**P.S.** Возможно, в датасете находятся пустые данные. Проверьте эту гипотезу, и если надо, сделайте предобратоку датасета.\n",
        "\n",
        "\n",
        "`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики МФТИ.`\n",
        "\n",
        "Загрузим датасет и необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqkLTkFRfXvA"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
        "!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
        "!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1bkRoT38awp",
        "outputId": "ae910448-ad0f-4928-bdbc-2bcc5b4cddcf"
      },
      "source": [
        "!pip install urllib3==1.24.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: urllib3==1.24.3 in /usr/local/lib/python3.6/dist-packages (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-m993nLP5Ma",
        "outputId": "8ed68952-8f92-4f7c-c897-0baed00dc28a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 65.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fd394ad6f9636796e0ae54586148ebac0b38b39d4eee7df9e688f05714634f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXS1sdYZCluU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fc285c-996a-4279-befb-a0c17fb12aab"
      },
      "source": [
        "!pip install -Uq razdel allennlp torch fasttext OpenNMT-py networkx pymorphy2 nltk rouge==0.3.1 summa\n",
        "!pip install -Uq transformers youtokentome"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 512kB 18.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 30.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 50.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 53.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 57.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 53.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.2MB 63.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 62.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 536kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 63.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.3MB/s \n",
            "\u001b[?25h  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: botocore 1.19.30 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 13.3MB/s \n",
            "\u001b[31mERROR: allennlp 1.2.2 has requirement transformers<3.6,>=3.4, but you'll have transformers 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZ2UGS2DGjH"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
        "    assert shuffle != sort_by_date\n",
        "    records = []\n",
        "    with open(file_name, \"r\") as r:\n",
        "        for line in r:\n",
        "            records.append(eval(line)) # Simple hack\n",
        "    records = pd.DataFrame(records)\n",
        "    if sort_by_date:\n",
        "        records = records.sort(\"date\")\n",
        "    if shuffle:\n",
        "        records = records.sample(frac=1)\n",
        "    return records"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDp-BunEA91"
      },
      "source": [
        "train_records = read_gazeta_records(\"gazeta_train.txt\")\n",
        "val_records = read_gazeta_records(\"gazeta_val.txt\")\n",
        "test_records = read_gazeta_records(\"gazeta_test.txt\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAcVSli3r3S"
      },
      "source": [
        "## 1 задание: TextRank (порог: 0.35 BLEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7jAQp-_Ds98"
      },
      "source": [
        "TextRank - unsupervised метод для составления кратких выжимок из текста. \n",
        "Описание метода:\n",
        "\n",
        "1. Сплитим текст по предложениям\n",
        "2. Считаем \"похожесть\" предложений между собой\n",
        "3. Строим граф предложений с взвешенными ребрами\n",
        "4. С помощью алгоритм PageRank получаем наиболее важные предложения, на основе которых делаем summary.\n",
        "\n",
        "Функция похожести можно сделать и из нейросетевых(или около) моделек: FastText, ELMO и BERT. Выберете один метод, загрузите предобученную модель и с ее помощью для каждого предложениия сделайте sentence embedding. С помощью косинусной меры определяйте похожесть предложений.\n",
        "\n",
        "Предобученные модели можно взять по [ссылке](http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqN50Z006-Yl"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "def calc_scores(references, predictions, metric=\"all\"):\n",
        "    print(\"Count:\", len(predictions))\n",
        "    print(\"Ref:\", references[-1])\n",
        "    print(\"Hyp:\", predictions[-1])\n",
        "\n",
        "    if metric in (\"bleu\", \"all\"):\n",
        "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
        "    if metric in (\"rouge\", \"all\"):\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(predictions, references, avg=True)\n",
        "        print(\"ROUGE: \", scores)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_lFs7muYp-s",
        "outputId": "9b45c962-db69-4826-c6ef-4a6b05f5d8e4"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ffyiqeshssn12hz/pytorch_model.bin\n",
        "!wget https://www.dropbox.com/s/xpktqk8dw72c3zi/vocab.txt\n",
        "!wget https://www.dropbox.com/s/8bhs4mo4axwaipo/bert_config.json"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 21:11:56--  https://www.dropbox.com/s/ffyiqeshssn12hz/pytorch_model.bin\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ffyiqeshssn12hz/pytorch_model.bin [following]\n",
            "--2020-12-05 21:11:57--  https://www.dropbox.com/s/raw/ffyiqeshssn12hz/pytorch_model.bin\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com/cd/0/inline/BEhTbcbcpI-ekt2Q32K8GxssogyW2Gf4EQHn_Tpt_H_cWUnG3Qm1QBZ0UC9BAMFJH9AyR1buVwy_F3nj0DWcwL323c4HMnom8fOdwz2MyYRHOgtxyyh27Xn6Kktqs_I-ve4/file# [following]\n",
            "--2020-12-05 21:11:57--  https://ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com/cd/0/inline/BEhTbcbcpI-ekt2Q32K8GxssogyW2Gf4EQHn_Tpt_H_cWUnG3Qm1QBZ0UC9BAMFJH9AyR1buVwy_F3nj0DWcwL323c4HMnom8fOdwz2MyYRHOgtxyyh27Xn6Kktqs_I-ve4/file\n",
            "Resolving ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com (ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com (ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BEhMzrJBi5BCutgxxa5zcB9CAi3H98nJAdtbplUDDAJxWjqO41S6x7a4XF77GoNaNiq51pWuw2NU9llv41XHPqEvg22LgEQp9rI7_Evbq1XbWED2PRBZTnTBwWPE0J8ER3IxZGzTg276C_YNJNxut8SBLgGIMWwumBJoBHX34p3d4-zojUOWMEUZVMJacP8iaMHjwhT78Bb5MLMyoH7wixBU51ZcdLS9Q2YBqSjm7lArU8UpyEnXn2NlVFJ9BTFgxlpiEDdYHV0Rt7-aAznA0k9Q6amGZyMhHdcGnfI8Hz1X8PWHh7tIOQ_TqJv7LUaALo3g4JSUxsXToJH3tRIbfrviEBTncBflp14P64li_Oaoiw/file [following]\n",
            "--2020-12-05 21:11:59--  https://ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com/cd/0/inline2/BEhMzrJBi5BCutgxxa5zcB9CAi3H98nJAdtbplUDDAJxWjqO41S6x7a4XF77GoNaNiq51pWuw2NU9llv41XHPqEvg22LgEQp9rI7_Evbq1XbWED2PRBZTnTBwWPE0J8ER3IxZGzTg276C_YNJNxut8SBLgGIMWwumBJoBHX34p3d4-zojUOWMEUZVMJacP8iaMHjwhT78Bb5MLMyoH7wixBU51ZcdLS9Q2YBqSjm7lArU8UpyEnXn2NlVFJ9BTFgxlpiEDdYHV0Rt7-aAznA0k9Q6amGZyMhHdcGnfI8Hz1X8PWHh7tIOQ_TqJv7LUaALo3g4JSUxsXToJH3tRIbfrviEBTncBflp14P64li_Oaoiw/file\n",
            "Reusing existing connection to ucba7a05b5f27b8919b72bff1b75.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 714313247 (681M) [application/octet-stream]\n",
            "Saving to: ‘pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 681.22M  29.3MB/s    in 25s     \n",
            "\n",
            "2020-12-05 21:12:25 (26.8 MB/s) - ‘pytorch_model.bin’ saved [714313247/714313247]\n",
            "\n",
            "--2020-12-05 21:12:25--  https://www.dropbox.com/s/xpktqk8dw72c3zi/vocab.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/xpktqk8dw72c3zi/vocab.txt [following]\n",
            "--2020-12-05 21:12:25--  https://www.dropbox.com/s/raw/xpktqk8dw72c3zi/vocab.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com/cd/0/inline/BEiZxdUyNPAMgPsfe5vQ91-ep7r7o4AcQI4F43V9yllNKvrfv8wVBt75LRz-dTArTDxVM14xPhPv2ETDJoERD3F9ApvRr08pFU4nP4XzqSoSFjtwoMjISTUqchEJUSaUBjY/file# [following]\n",
            "--2020-12-05 21:12:26--  https://uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com/cd/0/inline/BEiZxdUyNPAMgPsfe5vQ91-ep7r7o4AcQI4F43V9yllNKvrfv8wVBt75LRz-dTArTDxVM14xPhPv2ETDJoERD3F9ApvRr08pFU4nP4XzqSoSFjtwoMjISTUqchEJUSaUBjY/file\n",
            "Resolving uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com (uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com (uc67db071d12ee8bd7d0f6bab2f2.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1649718 (1.6M) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>]   1.57M  2.67MB/s    in 0.6s    \n",
            "\n",
            "2020-12-05 21:12:27 (2.67 MB/s) - ‘vocab.txt’ saved [1649718/1649718]\n",
            "\n",
            "--2020-12-05 21:12:27--  https://www.dropbox.com/s/8bhs4mo4axwaipo/bert_config.json\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/8bhs4mo4axwaipo/bert_config.json [following]\n",
            "--2020-12-05 21:12:27--  https://www.dropbox.com/s/raw/8bhs4mo4axwaipo/bert_config.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com/cd/0/inline/BEhKc6rtRI566RP58cgEqxeA8Yzmvy6JCU08rp0_9j9BZnpxMgO0EBzOcfE-DF3MKnjiAa_u9rJDVZ_-hRTg7DmzGjHJbNxdaUA07NLbOZIobSLg3u6tVJcVXd-_IjUHxmQ/file# [following]\n",
            "--2020-12-05 21:12:28--  https://uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com/cd/0/inline/BEhKc6rtRI566RP58cgEqxeA8Yzmvy6JCU08rp0_9j9BZnpxMgO0EBzOcfE-DF3MKnjiAa_u9rJDVZ_-hRTg7DmzGjHJbNxdaUA07NLbOZIobSLg3u6tVJcVXd-_IjUHxmQ/file\n",
            "Resolving uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com (uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com (uc63a33a6c885d7b947740f4ae4d.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 521 [text/plain]\n",
            "Saving to: ‘bert_config.json’\n",
            "\n",
            "bert_config.json    100%[===================>]     521  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-12-05 21:12:28 (184 KB/s) - ‘bert_config.json’ saved [521/521]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKqz5IESYc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c6e25c0c8a3f44f2b52ddec78c53afe9",
            "42a1244a8d094d35912e063d147690b9",
            "c0fed38155ef473692c9a5db19e8ce90",
            "44cc66dc62b64680a1d7f73460311f97",
            "d8aaf328e39d4fb1bade97a1070c0944",
            "78427412ef8c4f64a5cb4316fd9e51db",
            "152f6c043c24407692134d38786fc661",
            "2e80805cbb4543f691f8ebeab1cbc99c"
          ]
        },
        "outputId": "554aebf6-16d1-42a8-b22c-28207d2d1e38"
      },
      "source": [
        "import transformers\n",
        "import torch\n",
        "class Seq2Vec():\n",
        "  def __init__(self,vocabName=\"vocab.txt\",bertConfig = \"bert_config.json\",bertName=\"pytorch_model.bin\", tokenizer_name = \"bert-base-multilingual-cased\"):\n",
        "    self.tokenizer = transformers.BertTokenizer.from_pretrained(tokenizer_name)\n",
        "    self.simple_vocab = {}\n",
        "    with open(vocabName,\"r\") as vocab:\n",
        "      all_text = vocab.read().split('\\n')\n",
        "      for i in range(len(all_text)):\n",
        "        self.simple_vocab[all_text[i]] = i\n",
        "    self.model = transformers.BertModel.from_pretrained(bertName, config = bertConfig, output_hidden_states=True)\n",
        "    #self.model = transformers.BertModel.from_pretrained(tokenizer_name,output_hidden_states=True)\n",
        "    self.model.to(\"cuda\")\n",
        "    self.model.eval()\n",
        "    self.data = {}\n",
        "\n",
        "    self.PAD = self.simple_vocab['[PAD]']\n",
        "    self.SEP = self.simple_vocab['[SEP]']\n",
        "    self.CLS = self.simple_vocab['[CLS]']\n",
        "    #self.tokenizer.to(\"cuda\")\n",
        "  def get_ids(self,tokens):\n",
        "    result = [self.CLS]\n",
        "    for k in tokens:\n",
        "      if (k in self.simple_vocab):\n",
        "         result.append(self.simple_vocab[k])\n",
        "      else: result.append(0)\n",
        "    #print(tokens, result)\n",
        "    result.append(self.SEP)\n",
        "    if (result == [self.CLS]): return [self.CLS,0,self.SEP]\n",
        "    return result\n",
        "  def get_vec(self,sequence):\n",
        "    with torch.no_grad():\n",
        "      if(\"|\".join(sequence) in self.data):\n",
        "        #print(\"YES\")\n",
        "        return self.data[\"|\".join(sequence)]\n",
        "      tokens = self.tokenizer.tokenize(\" \".join(sequence))\n",
        "      #tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "      #tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "      tokens = self.get_ids(tokens)\n",
        "      #print(tokens)\n",
        "      #print(tokens)\n",
        "      embeddings = self.model(torch.LongTensor([tokens]).to(\"cuda\")).last_hidden_state[:,0,:]\n",
        "      \n",
        "      #print(embeddings.shape)\n",
        "      res = embeddings.cpu().reshape(-1).detach().numpy()\n",
        "      #print(res.shape)\n",
        "      self.data[\"|\".join(sequence)] = res\n",
        "      return res\n",
        "#path = 'navec_news_v1_1B_250K_300d_100q.tar'\n",
        "#model = navec\n",
        "model = Seq2Vec()\n",
        "#from navec import Navec\n",
        "#path = 'navec_news_v1_1B_250K_300d_100q.tar'\n",
        "#navec = Navec.load(path) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6e25c0c8a3f44f2b52ddec78c53afe9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2GwyRrMPAzS"
      },
      "source": [
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import razdel\n",
        "from numpy.linalg import norm\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "#import transformers\n",
        "#model = transformers.BertTokenizer.from_pretrained(\"RuBERT.gz\", cache_dir=None)\n",
        "#print(model)\n",
        "ITERS = 0\n",
        "def your_super_words_similarity(words1, words2, embeddings = model):\n",
        "    #TEMP\n",
        "    global ITERS\n",
        "    ITERS+=1\n",
        "    v1, v2 = model.get_vec(words1), model.get_vec(words2)\n",
        "    v1 = v1/np.linalg.norm(v1)\n",
        "    v2 = v2/np.linalg.norm(v2)\n",
        "    if (ITERS%100000==0): print(ITERS)\n",
        "    #print(v1.dot(v2))\n",
        "    return v1.dot(v2)\n",
        "\n",
        "\n",
        "def gen_text_rank_summary(text, calc_similarity=your_super_words_similarity, summary_part=0.1, lower=True, morph=None):\n",
        "    '''\n",
        "    Составление summary с помощью TextRank\n",
        "    '''\n",
        "    # Разбиваем текст на предложения\n",
        "    sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
        "    n_sentences = len(sentences)\n",
        "\n",
        "    # Токенизируем предложения\n",
        "    sentences_words = [[token.text.lower() if lower else token.text for token in razdel.tokenize(sentence)] for sentence in sentences]\n",
        "\n",
        "    # При необходимости лемматизируем слова\n",
        "    if morph is not None:\n",
        "        sentences_words = [[morph.parse(word)[0].normal_form for word in words] for words in sentences_words]\n",
        "\n",
        "    # Для каждой пары предложений считаем близость\n",
        "    pairs = combinations(range(n_sentences), 2) \n",
        "    scores = [(i, j, calc_similarity(sentences_words[i], sentences_words[j])) for i, j in pairs]\n",
        "\n",
        "    # Строим граф с рёбрами, равными близости между предложениями\n",
        "    g = nx.Graph()\n",
        "    g.add_weighted_edges_from(scores)\n",
        "\n",
        "    # Считаем PageRank\n",
        "    pr = nx.pagerank_numpy(g)\n",
        "    result = [(i, pr[i], s) for i, s in enumerate(sentences) if i in pr]\n",
        "    result.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Выбираем топ предложений\n",
        "    n_summary_sentences = max(int(n_sentences * summary_part), 1)\n",
        "    result = result[:n_summary_sentences]\n",
        "\n",
        "    # Восстанавливаем оригинальный их порядок\n",
        "    result.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Восстанавливаем текст выжимки\n",
        "    predicted_summary = \" \".join([sentence for i, proba, sentence in result])\n",
        "    predicted_summary = predicted_summary.lower() if lower else predicted_summary\n",
        "    return predicted_summary\n",
        "\n",
        "def calc_text_rank_score(records, calc_similarity=your_super_words_similarity, summary_part=0.1, lower=True, nrows=1000, morph=None):\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    for text, summary in records[['text', 'summary']].values[:nrows]:\n",
        "        summary = summary if not lower else summary.lower()\n",
        "        references.append(summary)\n",
        "\n",
        "        predicted_summary = gen_text_rank_summary(text, calc_similarity, summary_part, lower, morph=morph)\n",
        "        text = text if not lower else text.lower()\n",
        "        predictions.append(predicted_summary)\n",
        "    \n",
        "    calc_scores(references, predictions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY8T3yth6-Yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d873f8-372c-4cae-f579-b34cf171755b"
      },
      "source": [
        "calc_text_rank_score(test_records, calc_similarity=your_super_words_similarity)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "600000\n",
            "700000\n",
            "Count: 1000\n",
            "Ref: поскольку сша и европа первыми ввели санкции против россии, то наша страна не станет в одностороннем порядке отменять их. и хотя с 2014 года, когда были введены первые санкции, в сельском хозяйстве появились новые товары отечественного производства, из-за девальвации рубля продукты сильно подорожали.\n",
            "Hyp: ключевым торговым партнером россии по-прежнему остается евросоюз. часть европейских поставок все равно достигают россии, но они стали осуществляться через минск. по его словам, геополитическая ситуация пока не позволяет это предпринять.\n",
            "BLEU:  0.35574776662378244\n",
            "ROUGE:  {'rouge-1': {'f': 0.14135560669836073, 'p': 0.144132094192153, 'r': 0.1463283864337989}, 'rouge-2': {'f': 0.025196085720647264, 'p': 0.02597286647977443, 'r': 0.025906311548126434}, 'rouge-l': {'f': 0.12320597926503388, 'p': 0.13125431533009035, 'r': 0.13292647573332}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdTrfxycB7cd"
      },
      "source": [
        "## 2 Задание: Extractive RNN (порог: 0.35 BLEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q7DeHDYFSjX"
      },
      "source": [
        "Второй метод, который вам предлагается улучшить – поиск предложений для summary с помощью RNN. В рассмотренной методе мы использовали LSTM для генерации sentence embedding. Попробуйте использовать другие архитектуры: CNN, Transformer; или добавьте предобученные модели, как и в первом задании.\n",
        "\n",
        "P.S. Тут предполагается, что придется изменять много кода в ячееках (например, поменять токенизацию). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZamxigdEc-"
      },
      "source": [
        "### Модель\n",
        "\n",
        "Картинка для привлечения внимания:\n",
        "\n",
        "![img](https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_398421%2Fimages%2Farchitecture.png)\n",
        "\n",
        "Статья с оригинальным методом:\n",
        "https://arxiv.org/pdf/1611.04230.pdf\n",
        "\n",
        "Список вдохновения: \n",
        "- https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b Пример того, как можно применять CNN в текстовых задачах\n",
        "- https://arxiv.org/pdf/1808.08745.pdf Очень крутой метод генерации summary без Transformers\n",
        "- https://towardsdatascience.com/super-easy-way-to-get-sentence-embedding-using-fasttext-in-python-a70f34ac5b7c – простой метод генерации sentence embedding\n",
        "- https://towardsdatascience.com/fse-2b1ffa791cf9 – Необычный метод генерации sentence embedding\n",
        "- https://github.com/UKPLab/sentence-transformers – BERT предобученный для sentence embedding\n",
        "\n",
        "P.S. Выше написанные ссылки нужны только для разогрева вашей фантазии, можно воспользоваться ими, а можно придумать свой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOH4ZbLkg_sM"
      },
      "source": [
        "Комментарий к заданию:\n",
        "Если посмотреть на архитектуру ~~почти~~ SummaRuNNer, то в ней есть два главных элемента: первая часть, которая читает предложения и возвращает векторы на каждое предложение, и вторая, которая выбирает предложения для суммаризации. Вторую часть мы не трогаем, а первую меняем. На что меняем – как вы решите. Главное: она должна иметь хорошее качество и встроиться в текущую модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxsc0Orf8hGq"
      },
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
        "    '''\n",
        "    Жадное построение oracle summary\n",
        "    '''\n",
        "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
        "    # Делим текст на предложения\n",
        "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "    n_sentences = len(sentences)\n",
        "    oracle_summary_sentences = set()\n",
        "    score = -1.0\n",
        "    summaries = []\n",
        "    for _ in range(min(n_sentences, 2)):\n",
        "        for i in range(n_sentences):\n",
        "            if i in oracle_summary_sentences:\n",
        "                continue\n",
        "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
        "            # Добавляем какое-то предложения к уже существующему summary\n",
        "            current_summary_sentences.add(i)\n",
        "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
        "            # Считаем метрики\n",
        "            current_score = calc_score(current_summary, gold_summary)\n",
        "            summaries.append((current_score, current_summary_sentences))\n",
        "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
        "        # Иначе на этом заканчиваем\n",
        "        best_summary_score, best_summary_sentences = max(summaries)\n",
        "        if best_summary_score <= score:\n",
        "            break\n",
        "        oracle_summary_sentences = best_summary_sentences\n",
        "        score = best_summary_score\n",
        "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
        "    return oracle_summary, oracle_summary_sentences\n",
        "\n",
        "def calc_single_score(pred_summary, gold_summary, rouge):\n",
        "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T_ak-KDB8rp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "bc293a8439b1451bad6cd6f74d2e52d8",
            "7928a138685e439cba0ff09f397d8523",
            "a73c7dc8446a4d398b82303deacfba5e",
            "c61b9bf96be14d528ba6a9af218a3848",
            "4419fde47c014b95ad96aff1ca0c508a",
            "f3a9a2d66ba14bc887e86e60a913a5c0",
            "e2d296265821476dbb03a4ae248ec9f2",
            "b1a8d0dd7d5f4ff89fadc333f195a04a"
          ]
        },
        "outputId": "847e6c87-3dd2-4da3-98ba-ed7aa97228db"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "def calc_oracle_score(records, nrows=1000, lower=True):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    rouge = Rouge()\n",
        "  \n",
        "    for text, summary in tqdm(records[['text', 'summary']].values[:nrows]):\n",
        "        summary = summary if not lower else summary.lower()\n",
        "        references.append(summary)\n",
        "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
        "        predictions.append(predicted_summary)\n",
        "\n",
        "    calc_scores(references, predictions)\n",
        "\n",
        "calc_oracle_score(test_records)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc293a8439b1451bad6cd6f74d2e52d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Count: 1000\n",
            "Ref: телеведущий дмитрий шепелев рассказал о непростых отношениях с семьей своей покойной возлюбленной жанны фриске. по словам шоумена, суд постановил родителям певицы видеться с внуком по полтора часа два раза в месяц, но они ни разу к нему не приехали.\n",
            "Hyp: ведущий передачи «на самом деле» дмитрий шепелев рассказал об отношениях с семьей скончавшейся от рака певицы жанны фриске. он подчеркнул, что, согласно постановлению суда, родители фриске могут видеться с внуком по полтора часа два раза в месяц.\n",
            "BLEU:  0.42589664646845066\n",
            "ROUGE:  {'rouge-1': {'f': 0.3490451460584015, 'p': 0.42940880325806274, 'r': 0.3121174553668983}, 'rouge-2': {'f': 0.19940504508059612, 'p': 0.2527668794480655, 'r': 0.17716857908080433}, 'rouge-l': {'f': 0.29934515952910823, 'p': 0.4004867192385652, 'r': 0.2894968565537724}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWgjewfWrbJZ"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код загрузки токенизатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qIRKm4TCHzN"
      },
      "source": [
        "import os\n",
        "\n",
        "import youtokentome as yttm\n",
        "\n",
        "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=30000, lower=True):\n",
        "    temp_file_name = \"temp.txt\"\n",
        "    with open(temp_file_name, \"w\") as temp:\n",
        "        for text, summary in records[['text', 'summary']].values:\n",
        "            if lower:\n",
        "                summary = summary.lower()\n",
        "                text = text.lower()\n",
        "            if not text or not summary:\n",
        "                continue\n",
        "            temp.write(text + \"\\n\")\n",
        "            temp.write(summary + \"\\n\")\n",
        "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n",
        "\n",
        "train_bpe(train_records, \"BPE_model.bin\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAkZ2f5LhWwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d673f27d-453a-4eee-e9c7-f34d0c15e05a"
      },
      "source": [
        "bpe_processor = yttm.BPE('BPE_model.bin')\n",
        "bpe_processor.encode([\"октябрь богат на изменения\"], output_type=yttm.OutputType.SUBWORD)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁октябрь', '▁богат', '▁на', '▁изменения']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkUL_YIGp-S"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код словаря"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhQYN1beiVEC"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, bpe_processor):\n",
        "        self.index2word = bpe_processor.vocab()\n",
        "        self.word2index = {w: i for i, w in enumerate(self.index2word)}\n",
        "        self.word2count = Counter()\n",
        "\n",
        "    def get_pad(self):\n",
        "        return self.word2index[\"<PAD>\"]\n",
        "\n",
        "    def get_sos(self):\n",
        "        return self.word2index[\"<SOS>\"]\n",
        "\n",
        "    def get_eos(self):\n",
        "        return self.word2index[\"<EOS>\"]\n",
        "\n",
        "    def get_unk(self):\n",
        "        return self.word2index[\"<UNK>\"]\n",
        "    \n",
        "    def has_word(self, word) -> bool:\n",
        "        return word in self.word2index\n",
        "\n",
        "    def get_index(self, word):\n",
        "        if word in self.word2index:\n",
        "            return self.word2index[word]\n",
        "        return self.get_unk()\n",
        "\n",
        "    def get_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def is_empty(self):\n",
        "        empty_size = 4\n",
        "        return self.size() <= empty_size\n",
        "\n",
        "    def reset(self):\n",
        "        self.word2count = Counter()\n",
        "        self.index2word = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "        self.word2index = {word: index for index, word in enumerate(self.index2word)}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qvZtNcOifAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f5f4d2-882a-4f53-f94e-c8bee42ce78c"
      },
      "source": [
        "vocabulary = Vocabulary(bpe_processor)\n",
        "vocabulary.size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdb-39jO-72q"
      },
      "source": [
        "from rouge import Rouge\n",
        "import razdel\n",
        "\n",
        "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
        "    rouge = Rouge()\n",
        "    sentences_ = []\n",
        "    oracle_sentences_ = []\n",
        "    oracle_summary_ = []\n",
        "    if nrows is not None:\n",
        "        records = records.iloc[:nrows].copy()\n",
        "    else:\n",
        "        records = records.copy()\n",
        "\n",
        "    for text, summary in tqdm(records[['text', 'summary']].values):\n",
        "        summary = summary.lower() if lower else summary\n",
        "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
        "                                                                         lower=lower, max_sentences=max_sentences)\n",
        "        sentences_ += [sentences]\n",
        "        oracle_sentences_ += [list(sentences_indicies)]\n",
        "        oracle_summary_ += [oracle_summary]\n",
        "    records['sentences'] = sentences_\n",
        "    records['oracle_sentences'] = oracle_sentences_\n",
        "    records['oracle_summary'] = oracle_summary_\n",
        "    return records\n",
        "\n",
        "# ext_train_records = add_oracle_summary_to_records(train_records, nrows=30000)\n",
        "# ext_val_records = add_oracle_summary_to_records(val_records, nrows=None)\n",
        "# ext_test_records = add_oracle_summary_to_records(test_records, nrows=None)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg2zO9X_6-Yl"
      },
      "source": [
        "Используй `pickle` для сохранения записей, чтобы потом не пересоздавать их потом. Если решаешь задание в колабе, можешь подключить свой гугл диск и сохранить данные в нём."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9aGF3PJfKFd",
        "outputId": "f288be11-d2b2-4054-f5de-2ebe9c563bb2"
      },
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pickle_train = open(\"/content/drive/MyDrive/train_records.bin\",\"rb\")\n",
        "pickle_test = open(\"/content/drive/MyDrive/test_records.bin\",\"rb\")\n",
        "pickle_valid = open(\"/content/drive/MyDrive/val_records.bin\",\"rb\")\n",
        "\n",
        "ext_test_records  = pickle.load(pickle_test)\n",
        "ext_val_records = pickle.load(pickle_valid)\n",
        "ext_train_records = pickle.load(pickle_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKYuYZQy6-Yl"
      },
      "source": [
        "# import pickle\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# with open(\"train_records.bin\", 'wb') as file:\n",
        "#     pickle.save(file, ext_train_records)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlXXc8qUHC5m"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код генератора датасета и батчевалки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNyxstTChK3C"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import razdel\n",
        "import torch\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "class ExtDataset(data.Dataset):\n",
        "    def __init__(self, records, vocabulary, bpe_processor, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
        "        self.records = records\n",
        "        self.num_samples = records.shape[0]\n",
        "        self.bpe_processor = bpe_processor\n",
        "        self.lower = lower\n",
        "        self.rouge = Rouge()\n",
        "        self.vocabulary = vocabulary\n",
        "        self.max_sentences = max_sentences\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        self.device = device\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.records.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_record = self.records.iloc[idx]\n",
        "        inputs = list(map(lambda x: x[:self.max_sentence_length], self.bpe_processor.encode(cur_record['sentences'], output_type=yttm.OutputType.ID)))\n",
        "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
        "        return {'inputs': inputs, 'outputs': outputs}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvARjudojEDD"
      },
      "source": [
        "# Это батчевалка\n",
        "def collate_fn(records):\n",
        "    max_length = max(len(sentence) for record in records for sentence in record['inputs'])\n",
        "    max_sentences = max(len(record['outputs']) for record in records)\n",
        "\n",
        "    new_inputs = torch.zeros((len(records), max_sentences, max_length))\n",
        "    new_outputs = torch.zeros((len(records), max_sentences))\n",
        "    for i, record in enumerate(records):\n",
        "        for j, sentence in enumerate(record['inputs']):\n",
        "            new_inputs[i, j, :len(sentence)] += np.array(sentence)\n",
        "        new_outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
        "    return {'features': new_inputs.type(torch.LongTensor), 'targets': new_outputs}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWlf7XdheJUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff404134-6ed6-4ffd-e4c9-bbc4b3e94791"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "\n",
        "class SentenceEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n",
        "        super().__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        embedded = self.embedding_layer(inputs)\n",
        "        outputs, _ = self.rnn_layer(embedded, hidden)\n",
        "        sentences_embeddings = torch.mean(outputs, 1)\n",
        "\n",
        "        return sentences_embeddings\n",
        "\n",
        "class SentenceTaggerRNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 token_embedding_dim=256,\n",
        "                 sentence_encoder_hidden_size=256,\n",
        "                 hidden_size=256,\n",
        "                 bidirectional=True,\n",
        "                 sentence_encoder_n_layers=2,\n",
        "                 sentence_encoder_dropout=0.3,\n",
        "                 sentence_encoder_bidirectional=True,\n",
        "                 n_layers=1,\n",
        "                 dropout=0.3):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n",
        "                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n",
        "                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n",
        "        \n",
        "        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n",
        "                           bidirectional=bidirectional, batch_first=True)\n",
        "        \n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
        "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.tanh_layer = nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "\n",
        "        # [batch_size, seq num, seq_len]\n",
        "        batch_size = inputs.size(0)\n",
        "        sentences_count = inputs.size(1)\n",
        "        tokens_count = inputs.size(2)\n",
        "        inputs = inputs.reshape(-1, tokens_count)\n",
        "        # [batch_size * seq num, seq_len]\n",
        "\n",
        "        embedded_sentences = self.sentence_encoder(inputs)\n",
        "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
        "        # [batch_size, seq num, hidden_size]\n",
        "\n",
        "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        # [batch_size, seq num, hidden_size]\n",
        "\n",
        "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
        "        # [batch_size, hidden_size]\n",
        "\n",
        "        # W * h^T\n",
        "        content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n",
        "        # [batch_size, seq num]\n",
        "\n",
        "        # h^T * W * d\n",
        "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n",
        "\n",
        "        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, 1\n",
        "        return content + salience\n",
        "\n",
        "model = SentenceTaggerRNN(vocabulary.size())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2Gb6ODHHB_"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDW8raJeQxn"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "loaders = {\n",
        "    'train': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_train_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=64, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "    'valid': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_val_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=64, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "    'test': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_test_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=1, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "}\n",
        "\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# Maybe adding scheduler?"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH3kUHbu6-Yl"
      },
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.to(device)\n",
        "    pbar_loader = trange(len(loaders[\"train\"]) + len(loaders[\"valid\"]), desc=f\"Train Loss: {0}, Valid Loss: {0}\")\n",
        "    for e in trange(num_epochs, desc=\"Epoch\"):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "        train_it = 0\n",
        "        valid_it = 0\n",
        "        \n",
        "        model.train()\n",
        "        for batch in loaders[\"train\"]:\n",
        "            features = batch[\"features\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "            \n",
        "            logits = model(features)\n",
        "            \n",
        "            loss = criterion(logits, targets)\n",
        "            train_loss += loss.item()\n",
        "            train_it += 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Maybe adding scheduler?\n",
        "            \n",
        "            pbar_loader.update()\n",
        "            pbar_loader.set_description(\n",
        "                f\"Train Loss: {train_loss / train_it:.3}, Valid Loss: {0}\"\n",
        "            )\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in loaders[\"valid\"]:\n",
        "                features = batch[\"features\"].to(device)\n",
        "                targets = batch[\"targets\"].to(device)\n",
        "\n",
        "                logits = model(features)\n",
        "\n",
        "                loss = criterion(logits, targets)\n",
        "                valid_loss += loss.item()\n",
        "                valid_it += 1\n",
        "                \n",
        "                pbar_loader.update()\n",
        "                pbar_loader.set_description(\n",
        "                    f\"Train Loss: {train_loss / train_it:.3},\"\n",
        "                    f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
        "                )\n",
        "        print(\n",
        "            f\"Epoch {e}; Train Loss: {train_loss / train_it:.3},\"\n",
        "            f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
        "        )\n",
        "        pbar_loader.reset()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbjAk5aL6-Yl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "34d14220ad9546d7b96ff6de61a1e0a7",
            "b785efc4d7974acba2b009716bfd2926",
            "1e502179f7714f4ca281f383bf40a4f3",
            "fb1c6d9f0c5245d6b6f81d02cccffbb7",
            "0441c107c25a40b8953dd1b1c766b2f8",
            "d5c5471fb385486c82324087a5a32f63",
            "d9f58f788cac476684847a7e32e37941",
            "9b57d4e937474ab49477efea9100fdb1",
            "9f4d1c4c29f14ba487f674034c949676",
            "042caa39ec554633a21d284db09fcc8b",
            "976cdf106994467e9859d2e05fc9a304",
            "d5c8f1c15704469d91b06299aabb501f",
            "fc1009a6a8e14b66a9cf2e6f2ff712e0",
            "1fb66e6a9b284483bf9c4fe15cb92f85",
            "6cf1acb7d65e4f1e95fbc1bc725acf6d",
            "7a6b6d840b1645f28e1f1541299ab1d2"
          ]
        },
        "outputId": "9691c0b8-010d-43d0-8ab4-80b4ea320207"
      },
      "source": [
        "train()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34d14220ad9546d7b96ff6de61a1e0a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Train Loss: 0, Valid Loss: 0', max=552.0, style=ProgressS…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f4d1c4c29f14ba487f674034c949676",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Train Loss: 0.204, Valid Loss: 0.182\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwqhK2dyKuGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02798b6f-3b56-4a77-ba2d-dc03505b8a27"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "top_k = 3\n",
        "references = []\n",
        "predictions = []\n",
        "\n",
        "def postprocess(ref, hyp, is_multiple_ref=False, detokenize_after=False, tokenize_after=True):\n",
        "    if is_multiple_ref:\n",
        "        reference_sents = ref.split(\" s_s \")\n",
        "        decoded_sents = hyp.split(\"s_s\")\n",
        "        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n",
        "        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n",
        "        hyp = \" \".join(hyp)\n",
        "        ref = \" \".join(ref)\n",
        "    ref = ref.strip()\n",
        "    hyp = hyp.strip()\n",
        "    if detokenize_after:\n",
        "        hyp = punct_detokenize(hyp)\n",
        "        ref = punct_detokenize(ref)\n",
        "    if tokenize_after:\n",
        "        hyp = hyp.replace(\"@@UNKNOWN@@\", \"<unk>\")\n",
        "        hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
        "        ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
        "    return ref, hyp\n",
        "\n",
        "model.eval()\n",
        "for num, batch in enumerate(loaders[\"test\"]):\n",
        "\n",
        "    logits = model(batch[\"features\"].to(device))\n",
        "    in_summary = torch.argsort(logits, dim=1)[:, -top_k:]\n",
        "    for i in range(len(batch['targets'])):\n",
        "\n",
        "        summary = ext_test_records.iloc[i]['summary']\n",
        "        summary = summary.lower()\n",
        "        predicted_summary = ' '.join([ext_test_records.iloc[i]['sentences'][idx] for idx in in_summary[i].sort()[0] if idx < len(ext_test_records.iloc[i]['sentences'])])\n",
        "        summary, predicted_summary = postprocess(summary, predicted_summary)\n",
        "\n",
        "        references.append(summary)\n",
        "        predictions.append(predicted_summary)\n",
        "\n",
        "calc_scores(references, predictions)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count: 5770\n",
            "Ref: в акваторию черного моря зашел американский эсминец ross , у которого на вооружении имеются ракеты и средства пво . вместе с тем неизвестно , куда он следует . пока он находится в черноморской акватории , его сопровождает российское судно « вышний волочек » .\n",
            "Hyp: в юго-западную часть акватории черного моря вошел американский эсминец ross . об этом сообщили в национальном центре управления обороной , передает риа « новости » . там отметили , что корабль идет в сопровождении российского судна черноморского флота « вышний волочек » .\n",
            "BLEU:  0.5548088998026359\n",
            "ROUGE:  {'rouge-1': {'f': 0.33333332833717866, 'p': 0.34285714285715496, 'r': 0.3243243243243641}, 'rouge-2': {'f': 0.15189872917801497, 'p': 0.15384615384616374, 'r': 0.14999999999998517}, 'rouge-l': {'f': 0.3328201180390804, 'p': 0.34285714285715496, 'r': 0.3243243243243641}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kc0etEGfJ0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}